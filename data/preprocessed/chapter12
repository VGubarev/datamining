Chapter 12 Outlier Detection

12.1

Outliers and Outlier Analysis
Let us first define what outliers are, categorize the different types of outliers, and then
discuss the challenges in outlier detection at a general level.

12.1.1 What Are Outliers?
Assume that a given statistical process is used to generate a set of data objects. An outlier
is a data object that deviates significantly from the rest of the objects, as if it were generated by a different mechanism. For ease of presentation within this chapter, we may
refer to data objects that are not outliers as “normal” or expected data. Similarly, we may
refer to outliers as “abnormal” data.
Example 12.1 Outliers. In Figure 12.1, most objects follow a roughly Gaussian distribution. However,
the objects in region R are significantly different. It is unlikely that they follow the same
distribution as the other objects in the data set. Thus, the objects in R are outliers in the
data set.
Outliers are different from noisy data. As mentioned in Chapter 3, noise is a random error or variance in a measured variable. In general, noise is not interesting in
data analysis, including outlier detection. For example, in credit card fraud detection,
a customer’s purchase behavior can be modeled as a random variable. A customer may
generate some “noise transactions” that may seem like “random errors” or “variance,”
such as by buying a bigger lunch one day, or having one more cup of coffee than usual.
Such transactions should not be treated as outliers; otherwise, the credit card company
would incur heavy costs from verifying that many transactions. The company may also
lose customers by bothering them with multiple false alarms. As in many other data
analysis and data mining tasks, noise should be removed before outlier detection.
Outliers are interesting because they are suspected of not being generated by the same
mechanisms as the rest of the data. Therefore, in outlier detection, it is important to

R

Figure 12.1 The objects in region R are outliers.

12.1 Outliers and Outlier Analysis

545

justify why the outliers detected are generated by some other mechanisms. This is often
achieved by making various assumptions on the rest of the data and showing that the
outliers detected violate those assumptions significantly.
Outlier detection is also related to novelty detection in evolving data sets. For example,
by monitoring a social media web site where new content is incoming, novelty detection
may identify new topics and trends in a timely manner. Novel topics may initially appear
as outliers. To this extent, outlier detection and novelty detection share some similarity
in modeling and detection methods. However, a critical difference between the two is
that in novelty detection, once new topics are confirmed, they are usually incorporated
into the model of normal behavior so that follow-up instances are not treated as outliers
anymore.

12.1.2 Types of Outliers
In general, outliers can be classified into three categories, namely global outliers, contextual (or conditional) outliers, and collective outliers. Let’s examine each of these
categories.

Global Outliers
In a given data set, a data object is a global outlier if it deviates significantly from the rest
of the data set. Global outliers are sometimes called point anomalies, and are the simplest
type of outliers. Most outlier detection methods are aimed at finding global outliers.
Example 12.2 Global outliers. Consider the points in Figure 12.1 again. The points in region R significantly deviate from the rest of the data set, and hence are examples of global outliers.
To detect global outliers, a critical issue is to find an appropriate measurement of
deviation with respect to the application in question. Various measurements are proposed, and, based on these, outlier detection methods are partitioned into different
categories. We will come to this issue in detail later.
Global outlier detection is important in many applications. Consider intrusion detection in computer networks, for example. If the communication behavior of a computer
is very different from the normal patterns (e.g., a large number of packages is broadcast in a short time), this behavior may be considered as a global outlier and the
corresponding computer is a suspected victim of hacking. As another example, in trading transaction auditing systems, transactions that do not follow the regulations are
considered as global outliers and should be held for further examination.

Contextual Outliers
“The temperature today is 28◦ C. Is it exceptional (i.e., an outlier)?” It depends, for example, on the time and location! If it is in winter in Toronto, yes, it is an outlier. If it is a
summer day in Toronto, then it is normal. Unlike global outlier detection, in this case,

546

Chapter 12 Outlier Detection

whether or not today’s temperature value is an outlier depends on the context—the date,
the location, and possibly some other factors.
In a given data set, a data object is a contextual outlier if it deviates significantly
with respect to a specific context of the object. Contextual outliers are also known as
conditional outliers because they are conditional on the selected context. Therefore, in
contextual outlier detection, the context has to be specified as part of the problem definition. Generally, in contextual outlier detection, the attributes of the data objects in
question are divided into two groups:
Contextual attributes: The contextual attributes of a data object define the object’s
context. In the temperature example, the contextual attributes may be date and
location.
Behavioral attributes: These define the object’s characteristics, and are used to evaluate whether the object is an outlier in the context to which it belongs. In the
temperature example, the behavioral attributes may be the temperature, humidity,
and pressure.
Unlike global outlier detection, in contextual outlier detection, whether a data object
is an outlier depends on not only the behavioral attributes but also the contextual
attributes. A configuration of behavioral attribute values may be considered an outlier in
one context (e.g., 28◦ C is an outlier for a Toronto winter), but not an outlier in another
context (e.g., 28◦ C is not an outlier for a Toronto summer).
Contextual outliers are a generalization of local outliers, a notion introduced in
density-based outlier analysis approaches. An object in a data set is a local outlier if
its density significantly deviates from the local area in which it occurs. We will discuss
local outlier analysis in greater detail in Section 12.4.3.
Global outlier detection can be regarded as a special case of contextual outlier detection where the set of contextual attributes is empty. In other words, global outlier
detection uses the whole data set as the context. Contextual outlier analysis provides
flexibility to users in that one can examine outliers in different contexts, which can be
highly desirable in many applications.
Example 12.3 Contextual outliers. In credit card fraud detection, in addition to global outliers, an
analyst may consider outliers in different contexts. Consider customers who use more
than 90% of their credit limit. If one such customer is viewed as belonging to a group of
customers with low credit limits, then such behavior may not be considered an outlier.
However, similar behavior of customers from a high-income group may be considered
outliers if their balance often exceeds their credit limit. Such outliers may lead to business opportunities—raising credit limits for such customers can bring in new revenue.

12.1 Outliers and Outlier Analysis

547

The quality of contextual outlier detection in an application depends on the
meaningfulness of the contextual attributes, in addition to the measurement of the deviation of an object to the majority in the space of behavioral attributes. More often
than not, the contextual attributes should be determined by domain experts, which
can be regarded as part of the input background knowledge. In many applications, neither obtaining sufficient information to determine contextual attributes nor collecting
high-quality contextual attribute data is easy.
“How can we formulate meaningful contexts in contextual outlier detection?” A
straightforward method simply uses group-bys of the contextual attributes as contexts.
This may not be effective, however, because many group-bys may have insufficient data
and/or noise. A more general method uses the proximity of data objects in the space of
contextual attributes. We discuss this approach in detail in Section 12.4.

Collective Outliers
Suppose you are a supply-chain manager of AllElectronics. You handle thousands of
orders and shipments every day. If the shipment of an order is delayed, it may not be
considered an outlier because, statistically, delays occur from time to time. However,
you have to pay attention if 100 orders are delayed on a single day. Those 100 orders
as a whole form an outlier, although each of them may not be regarded as an outlier if
considered individually. You may have to take a close look at those orders collectively to
understand the shipment problem.
Given a data set, a subset of data objects forms a collective outlier if the objects as
a whole deviate significantly from the entire data set. Importantly, the individual data
objects may not be outliers.
Example 12.4 Collective outliers. In Figure 12.2, the black objects as a whole form a collective outlier
because the density of those objects is much higher than the rest in the data set. However,
every black object individually is not an outlier with respect to the whole data set.

Figure 12.2 The black objects form a collective outlier.

548

Chapter 12 Outlier Detection

Collective outlier detection has many important applications. For example, in
intrusion detection, a denial-of-service package from one computer to another is considered normal, and not an outlier at all. However, if several computers keep sending
denial-of-service packages to each other, they as a whole should be considered as a collective outlier. The computers involved may be suspected of being compromised by an
attack. As another example, a stock transaction between two parties is considered normal. However, a large set of transactions of the same stock among a small party in a short
period are collective outliers because they may be evidence of some people manipulating
the market.
Unlike global or contextual outlier detection, in collective outlier detection we have
to consider not only the behavior of individual objects, but also that of groups of
objects. Therefore, to detect collective outliers, we need background knowledge of the
relationship among data objects such as distance or similarity measurements between
objects.
In summary, a data set can have multiple types of outliers. Moreover, an object may
belong to more than one type of outlier. In business, different outliers may be used in
various applications or for different purposes. Global outlier detection is the simplest.
Context outlier detection requires background information to determine contextual
attributes and contexts. Collective outlier detection requires background information
to model the relationship among objects to find groups of outliers.

12.1.3 Challenges of Outlier Detection
Outlier detection is useful in many applications yet faces many challenges such as the
following:
Modeling normal objects and outliers effectively. Outlier detection quality highly
depends on the modeling of normal (nonoutlier) objects and outliers. Often, building a comprehensive model for data normality is very challenging, if not impossible.
This is partly because it is hard to enumerate all possible normal behaviors in an
application.
The border between data normality and abnormality (outliers) is often not clear
cut. Instead, there can be a wide range of gray area. Consequently, while some outlier detection methods assign to each object in the input data set a label of either
“normal” or “outlier,” other methods assign to each object a score measuring the
“outlier-ness” of the object.
Application-specific outlier detection. Technically, choosing the similarity/distance
measure and the relationship model to describe data objects is critical in outlier
detection. Unfortunately, such choices are often application-dependent. Different
applications may have very different requirements. For example, in clinic data analysis, a small deviation may be important enough to justify an outlier. In contrast, in
marketing analysis, objects are often subject to larger fluctuations, and consequently
a substantially larger deviation is needed to justify an outlier. Outlier detection’s high

12.2 Outlier Detection Methods

549

dependency on the application type makes it impossible to develop a universally
applicable outlier detection method. Instead, individual outlier detection methods
that are dedicated to specific applications must be developed.
Handling noise in outlier detection. As mentioned earlier, outliers are different from
noise. It is also well known that the quality of real data sets tends to be poor. Noise
often unavoidably exists in data collected in many applications. Noise may be present
as deviations in attribute values or even as missing values. Low data quality and
the presence of noise bring a huge challenge to outlier detection. They can distort
the data, blurring the distinction between normal objects and outliers. Moreover,
noise and missing data may “hide” outliers and reduce the effectiveness of outlier detection—an outlier may appear “disguised” as a noise point, and an outlier
detection method may mistakenly identify a noise point as an outlier.
Understandability. In some application scenarios, a user may want to not only
detect outliers, but also understand why the detected objects are outliers. To meet
the understandability requirement, an outlier detection method has to provide some
justification of the detection. For example, a statistical method can be used to justify the degree to which an object may be an outlier based on the likelihood that the
object was generated by the same mechanism that generated the majority of the data.
The smaller the likelihood, the more unlikely the object was generated by the same
mechanism, and the more likely the object is an outlier.
The rest of this chapter discusses approaches to outlier detection.

12.2

Outlier Detection Methods
There are many outlier detection methods in the literature and in practice. Here, we
present two orthogonal ways to categorize outlier detection methods. First, we categorize outlier detection methods according to whether the sample of data for analysis is
given with domain expert–provided labels that can be used to build an outlier detection
model. Second, we divide methods into groups according to their assumptions regarding
normal objects versus outliers.

12.2.1 Supervised, Semi-Supervised, and Unsupervised Methods
If expert-labeled examples of normal and/or outlier objects can be obtained, they can be
used to build outlier detection models. The methods used can be divided into supervised
methods, semi-supervised methods, and unsupervised methods.

Supervised Methods
Supervised methods model data normality and abnormality. Domain experts examine
and label a sample of the underlying data. Outlier detection can then be modeled as

550

Chapter 12 Outlier Detection

a classification problem (Chapters 8 and 9). The task is to learn a classifier that can
recognize outliers. The sample is used for training and testing. In some applications, the
experts may label just the normal objects, and any other objects not matching the model
of normal objects are reported as outliers. Other methods model the outliers and treat
objects not matching the model of outliers as normal.
Although many classification methods can be applied, challenges to supervised
outlier detection include the following:
The two classes (i.e., normal objects versus outliers) are imbalanced. That is, the population of outliers is typically much smaller than that of normal objects. Therefore,
methods for handling imbalanced classes (Section 8.6.5) may be used, such as oversampling (i.e., replicating) outliers to increase their distribution in the training set
used to construct the classifier. Due to the small population of outliers in data, the
sample data examined by domain experts and used in training may not even sufficiently represent the outlier distribution. The lack of outlier samples can limit the
capability of classifiers built as such. To tackle these problems, some methods “make
up” artificial outliers.
In many outlier detection applications, catching as many outliers as possible (i.e., the
sensitivity or recall of outlier detection) is far more important than not mislabeling
normal objects as outliers. Consequently, when a classification method is used for
supervised outlier detection, it has to be interpreted appropriately so as to consider
the application interest on recall.
In summary, supervised methods of outlier detection must be careful in how they
train and how they interpret classification rates due to the fact that outliers are rare in
comparison to the other data samples.

Unsupervised Methods
In some application scenarios, objects labeled as “normal” or “outlier” are not available.
Thus, an unsupervised learning method has to be used.
Unsupervised outlier detection methods make an implicit assumption: The normal
objects are somewhat “clustered.” In other words, an unsupervised outlier detection
method expects that normal objects follow a pattern far more frequently than outliers.
Normal objects do not have to fall into one group sharing high similarity. Instead, they
can form multiple groups, where each group has distinct features. However, an outlier is
expected to occur far away in feature space from any of those groups of normal objects.
This assumption may not be true all the time. For example, in Figure 12.2, the normal
objects do not share any strong patterns. Instead, they are uniformly distributed. The
collective outliers, however, share high similarity in a small area. Unsupervised methods
cannot detect such outliers effectively. In some applications, normal objects are diversely
distributed, and many such objects do not follow strong patterns. For instance, in some
intrusion detection and computer virus detection problems, normal activities are very
diverse and many do not fall into high-quality clusters. In such scenarios, unsupervised

12.2 Outlier Detection Methods

551

methods may have a high false positive rate—they may mislabel many normal objects
as outliers (intrusions or viruses in these applications), and let many actual outliers go
undetected. Due to the high similarity between intrusions and viruses (i.e., they have to
attack key resources in the target systems), modeling outliers using supervised methods
may be far more effective.
Many clustering methods can be adapted to act as unsupervised outlier detection
methods. The central idea is to find clusters first, and then the data objects not belonging to any cluster are detected as outliers. However, such methods suffer from two issues.
First, a data object not belonging to any cluster may be noise instead of an outlier. Second, it is often costly to find clusters first and then find outliers. It is usually assumed
that there are far fewer outliers than normal objects. Having to process a large population of nontarget data entries (i.e., the normal objects) before one can touch the real
meat (i.e., the outliers) can be unappealing. The latest unsupervised outlier detection
methods develop various smart ideas to tackle outliers directly without explicitly and
completely finding clusters. You will learn more about these techniques in Sections 12.4
and 12.5 on proximity-based and clustering-based methods, respectively.

Semi-Supervised Methods
In many applications, although obtaining some labeled examples is feasible, the number
of such labeled examples is often small. We may encounter cases where only a small set
of the normal and/or outlier objects are labeled, but most of the data are unlabeled.
Semi-supervised outlier detection methods were developed to tackle such scenarios.
Semi-supervised outlier detection methods can be regarded as applications of semisupervised learning methods (Section 9.7.2). For example, when some labeled normal
objects are available, we can use them, together with unlabeled objects that are close by,
to train a model for normal objects. The model of normal objects then can be used to
detect outliers—those objects not fitting the model of normal objects are classified as
outliers.
If only some labeled outliers are available, semi-supervised outlier detection is trickier. A small number of labeled outliers are unlikely to represent all the possible outliers.
Therefore, building a model for outliers based on only a few labeled outliers is unlikely
to be effective. To improve the quality of outlier detection, we can get help from models
for normal objects learned from unsupervised methods.
For additional information on semi-supervised methods, interested readers are
referred to the bibliographic notes at the end of this chapter (Section 12.11).

12.2.2 Statistical Methods, Proximity-Based Methods,
and Clustering-Based Methods
As discussed in Section 12.1, outlier detection methods make assumptions about outliers
versus the rest of the data. According to the assumptions made, we can categorize outlier
detection methods into three types: statistical methods, proximity-based methods, and
clustering-based methods.

552

Chapter 12 Outlier Detection

Statistical Methods
Statistical methods (also known as model-based methods) make assumptions of
data normality. They assume that normal data objects are generated by a statistical
(stochastic) model, and that data not following the model are outliers.
Example 12.5 Detecting outliers using a statistical (Gaussian) model. In Figure 12.1, the data points
except for those in region R fit a Gaussian distribution gD , where for a location x in the
data space, gD (x) gives the probability density at x. Thus, the Gaussian distribution gD
can be used to model the normal data, that is, most of the data points in the data set. For
each object y in region, R, we can estimate gD ( y), the probability that this point fits the
Gaussian distribution. Because gD ( y) is very low, y is unlikely generated by the Gaussian
model, and thus is an outlier.
The effectiveness of statistical methods highly depends on whether the assumptions
made for the statistical model hold true for the given data. There are many kinds of
statistical models. For example, the statistic models used in the methods may be parametric or nonparametric. Statistical methods for outlier detection are discussed in detail
in Section 12.3.

Proximity-Based Methods
Proximity-based methods assume that an object is an outlier if the nearest neighbors
of the object are far away in feature space, that is, the proximity of the object to its
neighbors significantly deviates from the proximity of most of the other objects to their
neighbors in the same data set.
Example 12.6 Detecting outliers using proximity. Consider the objects in Figure 12.1 again. If we
model the proximity of an object using its three nearest neighbors, then the objects
in region R are substantially different from other objects in the data set. For the two
objects in R, their second and third nearest neighbors are dramatically more remote
than those of any other objects. Therefore, we can label the objects in R as outliers based
on proximity.
The effectiveness of proximity-based methods relies heavily on the proximity (or distance) measure used. In some applications, such measures cannot be easily obtained.
Moreover, proximity-based methods often have difficulty in detecting a group of outliers
if the outliers are close to one another.
There are two major types of proximity-based outlier detection, namely distancebased and density-based outlier detection. Proximity-based outlier detection is discussed
in Section 12.4.

Clustering-Based Methods
Clustering-based methods assume that the normal data objects belong to large and
dense clusters, whereas outliers belong to small or sparse clusters, or do not belong to
any clusters.

12.3 Statistical Approaches

553

Example 12.7 Detecting outliers using clustering. In Figure 12.1, there are two clusters. Cluster C1
contains all the points in the data set except for those in region R. Cluster C2 is tiny,
containing just two points in R. Cluster C1 is large in comparison to C2 . Therefore, a
clustering-based method asserts that the two objects in R are outliers.
There are many clustering methods, as discussed in Chapters 10 and 11. Therefore, there are many clustering-based outlier detection methods as well. Clustering is an
expensive data mining operation. A straightforward adaptation of a clustering method
for outlier detection can be very costly, and thus does not scale up well for large data
sets. Clustering-based outlier detection methods are discussed in detail in Section 12.5.

12.3

Statistical Approaches
As with statistical methods for clustering, statistical methods for outlier detection make
assumptions about data normality. They assume that the normal objects in a data set are
generated by a stochastic process (a generative model). Consequently, normal objects
occur in regions of high probability for the stochastic model, and objects in the regions
of low probability are outliers.
The general idea behind statistical methods for outlier detection is to learn a generative model fitting the given data set, and then identify those objects in low-probability
regions of the model as outliers. However, there are many different ways to learn generative models. In general, statistical methods for outlier detection can be divided into two
major categories: parametric methods and nonparametric methods, according to how the
models are specified and learned.
A parametric method assumes that the normal data objects are generated by a parametric distribution with parameter 2. The probability density function of the parametric
distribution f (x, 2) gives the probability that object x is generated by the distribution.
The smaller this value, the more likely x is an outlier.
A nonparametric method does not assume an a priori statistical model. Instead, a
nonparametric method tries to determine the model from the input data. Note that
most nonparametric methods do not assume that the model is completely parameterfree. (Such an assumption would make learning the model from data almost mission
impossible.) Instead, nonparametric methods often take the position that the number and nature of the parameters are flexible and not fixed in advance. Examples of
nonparametric methods include histogram and kernel density estimation.

12.3.1 Parametric Methods
In this subsection, we introduce several simple yet practical parametric methods for
outlier detection. We first discuss methods for univariate data based on normal distribution. We then discuss how to handle multivariate data using multiple parametric
distributions.

554

Chapter 12 Outlier Detection

Detection of Univariate Outliers Based
on Normal Distribution
Data involving only one attribute or variable are called univariate data. For simplicity,
we often choose to assume that data are generated from a normal distribution. We can
then learn the parameters of the normal distribution from the input data, and identify
the points with low probability as outliers.
Let’s start with univariate data. We will try to detect outliers by assuming the data
follow a normal distribution.
Example 12.8 Univariate outlier detection using maximum likelihood. Suppose a city’s average temperature values in July in the last 10 years are, in value-ascending order, 24.0◦ C, 28.9◦ C,
28.9◦ C, 29.0◦ C, 29.1◦ C, 29.1◦ C, 29.2◦ C, 29.2◦ C, 29.3◦ C, and 29.4◦ C. Let’s assume that
the average temperature follows a normal distribution, which is determined by two
parameters: the mean, µ, and the standard deviation, σ .
We can use the maximum likelihood method to estimate the parameters µ and σ . That
is, we maximize the log-likelihood function
ln L(µ, σ 2 ) =

n
X
i=1

n

n
n
1 X
ln f (xi |(µ, σ 2 )) = − ln(2π ) − ln σ 2 − 2
(xi − µ)2 , (12.1)
2
2
2σ
i=1

where n is the total number of samples, which is 10 in this example.
Taking derivatives with respect to µ and σ 2 and solving the resulting system of firstorder conditions leads to the following maximum likelihood estimates:
n

1X
µ̂ = x =
xi
n

(12.2)

i=1

n

σ̂ 2 =

1X
(xi − x)2 .
n

(12.3)

i=1

In this example, we have
µ̂ =

24.0 + 28.9 + 28.9 + 29.0 + 29.1 + 29.1 + 29.2 + 29.2 + 29.3 + 29.4
= 28.61
10

σ̂ 2 = ((24.1 − 28.61)2 + (28.9 − 28.61)2 + (28.9 − 28.61)2 + (29.0 − 28.61)2
+ (29.1 − 28.61)2 + (29.1 − 28.61)2 + (29.2 − 28.61)2 + (29.2 − 28.61)2
+ (29.3 − 28.61)2 + (29.4 − 28.61)2 )/10 w 2.29.
√
Accordingly, we have σ̂ = 2.29 = 1.51.
The most deviating value, 24.0◦ C, is 4.61◦ C away from the estimated mean. We
know that the µ ± 3σ region contains 99.7% data under the assumption of normal

12.3 Statistical Approaches

555

Outlier
Max
Q3
Median

Q1
Min
Outliers

Figure 12.3 Using a boxplot to visualize outliers.
◦
distribution. Because 4.61
1.51 = 3.04 > 3, the probability that the value 24.0 C is generated
by the normal distribution is less than 0.15%, and thus can be identified as an outlier.

Example 12.8 elaborates a simple yet practical outlier detection method. It simply
labels any object as an outlier if it is more than 3σ away from the mean of the estimated
distribution, where σ is the standard deviation.
Such straightforward methods for statistical outlier detection can also be used in
visualization. For example, the boxplot method (described in Chapter 2) plots the univariate input data using a five-number summary (Figure 12.3): the smallest nonoutlier
value (Min), the lower quartile (Q1), the median (Q2), the upper quartile (Q3), and
the largest nonoutlier value (Max). The interquantile range (IQR) is defined as Q3 − Q1.
Any object that is more than 1.5 × IQR smaller than Q1 or 1.5 × IQR larger than Q3 is
treated as an outlier because the region between Q1 − 1.5 × IQR and Q3 + 1.5 × IQR
contains 99.3% of the objects. The rationale is similar to using 3σ as the threshold for
normal distribution.
Another simple statistical method for univariate outlier detection using normal distribution is the Grubb’s test (also known as the maximum normed residual test). For each
object x in a data set, we define a z-score as
z=

|x − x̄|
,
s

(12.4)

where x̄ is the mean, and s is the standard deviation of the input data. An object x is an
outlier if
v
u
2
tα/(2N
N − 1u
),N −2
t
z≥ √
,
(12.5)
2
N
−
2
+
t
N
α/(2N ),N −2
2
where tα/(2N
),N −2 is the value taken by a t-distribution at a significance level of α/(2N ),
and N is the number of objects in the data set.

556

Chapter 12 Outlier Detection

Detection of Multivariate Outliers
Data involving two or more attributes or variables are multivariate data. Many univariate
outlier detection methods can be extended to handle multivariate data. The central idea
is to transform the multivariate outlier detection task into a univariate outlier detection
problem. Here, we use two examples to illustrate this idea.
Example 12.9 Multivariate outlier detection using the Mahalanobis distance. For a multivariate
data set, let ō be the mean vector. For an object, o, in the data set, the Mahalanobis
distance from o to ō is
MDist(o, ō) = (o − ō)T S−1 (o − ō),

(12.6)

where S is the covariance matrix.
MDist(o, ō) is a univariate variable, and thus Grubb’s test can be applied to this
measure. Therefore, we can transform the multivariate outlier detection tasks as
follows:
1. Calculate the mean vector from the multivariate data set.
2. For each object o, calculate MDist(o, ō), the Mahalanobis distance from o to ō.
3. Detect outliers in the transformed univariate data set, {MDist(o, ō)|o ∈ D}.
4. If MDist(o, ō) is determined to be an outlier, then o is regarded as an outlier as well.
Our second example uses the χ 2 -statistic to measure the distance between an object
to the mean of the input data set.
Example 12.10 Multivariate outlier detection using the χ 2 -statistic. The χ 2 -statistic can also be used
to capture multivariate outliers under the assumption of normal distribution. For an
object, o, the χ 2 -statistic is
χ2 =

n
X
(oi − Ei )2
i=1

Ei

,

(12.7)

where oi is the value of o on the ith dimension, Ei is the mean of the i-dimension
among all objects, and n is the dimensionality. If the χ 2 -statistic is large, the object
is an outlier.

Using a Mixture of Parametric Distributions
If we assume that the data were generated by a normal distribution, this works well in
many situations. However, this assumption may be overly simplified when the actual
data distribution is complex. In such cases, we instead assume that the data were
generated by a mixture of parametric distributions.

12.3 Statistical Approaches

557

o
C3

C1

C2

Figure 12.4 A complex data set.

Example 12.11 Multivariate outlier detection using multiple parametric distributions. Consider the
data set in Figure 12.4. There are two big clusters, C1 and C2 . To assume that the data
are generated by a normal distribution would not work well here. The estimated mean
is located between the two clusters and not inside any cluster. The objects between the
two clusters cannot be detected as outliers since they are close to the mean.
To overcome this problem, we can instead assume that the normal data objects are
generated by multiple normal distributions, two in this case. That is, we assume two
normal distributions, 21 (µ1 , σ1 ) and 22 (µ2 , σ2 ). For any object, o, in the data set, the
probability that o is generated by the mixture of the two distributions is given by
Pr(o|21 , 22 ) = f21 (o) + f22 (o),
where f21 and f22 are the probability density functions of 21 and 22 , respectively. We
can use the expectation-maximization (EM) algorithm (Chapter 11) to learn the parameters µ1 , σ1 , µ2 , σ2 from the data, as we do in mixture models for clustering. Each cluster
is represented by a learned normal distribution. An object, o, is detected as an outlier if
it does not belong to any cluster, that is, the probability is very low that it was generated
by the combination of the two distributions.
Example 12.12 Multivariate outlier detection using multiple clusters. Most of the data objects shown
in Figure 12.4 are in either C1 or C2 . Other objects, representing noise, are uniformly
distributed in the data space. A small cluster, C3 , is highly suspicious because it is not
close to either of the two major clusters, C1 and C2 . The objects in C3 should therefore
be detected as outliers.
Note that identifying the objects in C3 as outliers is difficult, whether or not we
assume that the given data follow a normal distribution or a mixture of multiple distributions. This is because the probability of the objects in C3 will be higher than some
of the noise objects, like o in Figure 12.4, due to a higher local density in C3 .

558

Chapter 12 Outlier Detection

To tackle the problem demonstrated in Example 12.12, we can assume that the normal data objects are generated by a normal distribution, or a mixture of normal distributions, whereas the outliers are generated by another distribution. Heuristically, we can
add constraints on the distribution that is generating outliers. For example, it is reasonable to assume that this distribution has a larger variance if the outliers are distributed in
a larger area. Technically, we can assign σoutlier = kσ , where k is a user-specified parameter and σ is the standard deviation of the normal distribution generating the normal
data. Again, the EM algorithm can be used to learn the parameters.

12.3.2 Nonparametric Methods
In nonparametric methods for outlier detection, the model of “normal data” is learned
from the input data, rather than assuming one a priori. Nonparametric methods often
make fewer assumptions about the data, and thus can be applicable in more scenarios.
Example 12.13 Outlier detection using a histogram. AllElectronics records the purchase amount
for every customer transaction. Figure 12.5 uses a histogram (refer to Chapters 2 and
3) to graph these amounts as percentages, given all transactions. For example, 60% of
the transaction amounts are between $0.00 and $1000.
We can use the histogram as a nonparametric statistical model to capture outliers. For
example, a transaction in the amount of $7500 can be regarded as an outlier because
only 1 − (60% + 20% + 10% + 6.7% + 3.1%) = 0.2% of transactions have an amount
higher than $5000. On the other hand, a transaction amount of $385 can be treated as
normal because it falls into the bin (or bucket) holding 60% of the transactions.

60%

20%
10%
0

0−1

6.7%

1−2
2−3
3−4
Amount per transaction

3.1%
4−5

× $1000

Figure 12.5 Histogram of purchase amounts in transactions.

12.3 Statistical Approaches

559

As illustrated in the previous example, the histogram is a frequently used nonparametric statistical model that can be used to detect outliers. The procedure involves the
following two steps.
Step 1: Histogram construction. In this step, we construct a histogram using the input
data (training data). The histogram may be univariate as in Example 12.13, or
multivariate if the input data are multidimensional.
Note that although nonparametric methods do not assume any a priori statistical model, they often do require user-specified parameters to learn models from
data. For example, to construct a good histogram, a user has to specify the type of
histogram (e.g., equal width or equal depth) and other parameters (e.g., the number
of bins in the histogram or the size of each bin). Unlike parametric methods, these
parameters do not specify types of data distribution (e.g., Gaussian).
Step 2: Outlier detection. To determine whether an object, o, is an outlier, we can check
it against the histogram. In the simplest approach, if the object falls in one of the
histogram’s bins, the object is regarded as normal. Otherwise, it is considered an
outlier.
For a more sophisticated approach, we can use the histogram to assign an outlier score to the object. In Example 12.13, we can let an object’s outlier score be the
inverse of the volume of the bin in which the object falls. For example, the outlier
1
score for a transaction amount of $7500 is 0.2%
= 500, and that for a transaction
1
amount of $385 is 60% = 1.67. The scores indicate that the transaction amount of
$7500 is much more likely to be an outlier than that of $385.
A drawback to using histograms as a nonparametric model for outlier detection is
that it is hard to choose an appropriate bin size. On the one hand, if the bin size is set too
small, many normal objects may end up in empty or rare bins, and thus be misidentified
as outliers. This leads to a high false positive rate and low precision. On the other hand,
if the bin size is set too high, outlier objects may infiltrate into some frequent bins and
thus be “disguised” as normal. This leads to a high false negative rate and low recall.
To overcome this problem, we can adopt kernel density estimation to estimate the
probability density distribution of the data. We treat an observed object as an indicator of high probability density in the surrounding region. The probability density at a
point depends on the distances from this point to the observed objects. We use a kernel
function to model the influence of a sample point within its neighborhood. A kernel
K () is a non-negative real-valued integrable function that satisfies the following two
conditions:
R +∞
−∞

K (u)du = 1.

K (−u) = K (u) for all values of u.
A frequently used kernel is a standard Gaussian function with mean 0 and variance 1:


2
x − xi
1 − (x−xi )
K
(12.8)
= √ e 2h2 .
h
2π

560

Chapter 12 Outlier Detection

Let x1 , . . . , xn be an independent and identically distributed sample of a random
variable f . The kernel density approximation of the probability density function is
n

1 X
fˆh (x) =
K
nh
i=1




x − xi
,
h

(12.9)

where K () is a kernel and h is the bandwidth serving as a smoothing parameter.
Once the probability density function of a data set is approximated through kernel
density estimation, we can use the estimated density function fˆ to detect outliers. For an
object, o, fˆ (o) gives the estimated probability that the object is generated by the stochastic process. If fˆ (o) is high, then the object is likely normal. Otherwise, o is likely an
outlier. This step is often similar to the corresponding step in parametric methods.
In summary, statistical methods for outlier detection learn models from data to distinguish normal data objects from outliers. An advantage of using statistical methods is
that the outlier detection may be statistically justifiable. Of course, this is true only if the
statistical assumption made about the underlying data meets the constraints in reality.
The data distribution of high-dimensional data is often complicated and hard
to fully understand. Consequently, statistical methods for outlier detection on highdimensional data remain a big challenge. Outlier detection for high-dimensional data
is further addressed in Section 12.8.
The computational cost of statistical methods depends on the models. When simple
parametric models are used (e.g., a Gaussian), fitting the parameters typically takes linear time. When more sophisticated models are used (e.g., mixture models, where the
EM algorithm is used in learning), approximating the best parameter values often takes
several iterations. Each iteration, however, is typically linear with respect to the data set’s
size. For kernel density estimation, the model learning cost can be up to quadratic. Once
the model is learned, the outlier detection cost is often very small per object.

12.4

Proximity-Based Approaches
Given a set of objects in feature space, a distance measure can be used to quantify the
similarity between objects. Intuitively, objects that are far from others can be regarded
as outliers. Proximity-based approaches assume that the proximity of an outlier object
to its nearest neighbors significantly deviates from the proximity of the object to most
of the other objects in the data set.
There are two types of proximity-based outlier detection methods: distance-based
and density-based methods. A distance-based outlier detection method consults the
neighborhood of an object, which is defined by a given radius. An object is then considered an outlier if its neighborhood does not have enough other points. A density-based
outlier detection method investigates the density of an object and that of its neighbors.
Here, an object is identified as an outlier if its density is relatively much lower than that
of its neighbors.
Let’s start with distance-based outliers.

12.4 Proximity-Based Approaches

561

12.4.1 Distance-Based Outlier Detection and a Nested
Loop Method
A representative method of proximity-based outlier detection uses the concept of
distance-based outliers. For a set, D, of data objects to be analyzed, a user can specify a distance threshold, r, to define a reasonable neighborhood of an object. For each
object, o, we can examine the number of other objects in the r-neighborhood of o. If
most of the objects in D are far from o, that is, not in the r-neighborhood of o, then o
can be regarded as an outlier.
Formally, let r (r ≥ 0) be a distance threshold and π (0 < π ≤ 1) be a fraction
threshold. An object, o, is a DB(r, π )-outlier if
k{o0 |dist(o, o0 ) ≤ r}k
≤ π,
kDk

(12.10)

where dist(·, ·) is a distance measure.
Equivalently, we can determine whether an object, o, is a DB(r, π )-outlier by checking
the distance between o and its k-nearest neighbor, ok , where k = dπkDke. Object o is an
outlier if dist(o, ok ) > r, because in such a case, there are fewer than k objects except for
o that are in the r-neighborhood of o.
“How can we compute DB(r, π )-outliers?” A straightforward approach is to use nested
loops to check the r-neighborhood for every object, as shown in Figure 12.6. For any
object, oi (1 ≤ i ≤ n), we calculate the distance between oi and the other object, and
count the number of other objects in the r-neighborhood of oi . Once we find π · n other
Algorithm: Distance-based outlier detection.
Input:
a set of objects D = {o1 , . . . , on }, threshold r (r > 0) and π (0 < π ≤ 1);
Output: DB(r, π ) outliers in D.
Method:
for i = 1 to n do
count ← 0
for j = 1 to n do
if i 6= j and dist(oi , oj ) ≤ r then
count ← count + 1
if count ≥ π · n then
exit {oi cannot be a DB(r, π) outlier}
endif
endif
endfor
print oi {oi is a DB(r, π) outlier according to (Eq. 12.10)}
endfor;

Figure 12.6 Nested loop algorithm for DB(r, π)-outlier detection.

562

Chapter 12 Outlier Detection

objects within a distance r from oi , the inner loop can be terminated because oi already
violates (Eq. 12.10), and thus is not a DB(r, π )-outlier. On the other hand, if the inner
loop completes for oi , this means that oi has less than π · n neighbors in a radius of r,
and thus is a DB(r, π)-outlier.
The straightforward nested loop approach takes O(n2 ) time. Surprisingly, the actual
CPU runtime is often linear with respect to the data set size. For most nonoutlier objects,
the inner loop terminates early when the number of outliers in the data set is small,
which should be the case most of the time. Correspondingly, only a small fraction of the
data set is examined.
When mining large data sets where the complete set of objects cannot be held in
main memory, the nested loop approach is still costly. Suppose the main memory has
m pages for the mining. Instead of conducting the inner loop object by object, in such
a case, the outer loop uses m − 1 pages to hold as many objects as possible and uses the
remaining one page to run the inner loop. The inner loop cannot stop until all objects
in the m − 1 pages are identified as not being outliers, which is very unlikely to happen.
Correspondingly, it is likely that the algorithm has to incur O(( nb )2 ) input/output (I/O)
cost, where b is the number of objects that can be held in one page.
The major cost in the nested loop method comes from two aspects. First, to check
whether an object is an outlier, the nested loop method tests the object against the
whole data set. To improve, we need to explore how to determine the outlierness of an
object from the neighbors that are close to the object. Second, the nested loop method
checks objects one by one. To improve, we should try to group objects according to
their proximity, and check the outlierness of objects group by group most of the time.
Section 12.4.2 introduces how to implement the preceding ideas.

12.4.2 A Grid-Based Method
CELL is a grid-based method for distance-based outlier detection. In this method, the
data space is partitioned into a multidimensional grid, where each cell is a hypercube
that has a diagonal of length 2r , where r is a distance threshold parameter. In other words,
r
if there are l dimensions, the length of each edge of a cell is √
.
2 l
Consider a 2-D data set, for example. Figure 12.7 shows part of the grid. The length
of each edge of a cell is √r .
2 2
Consider the cell C in Figure 12.7. The neighboring cells of C can be divided into
two groups. The cells immediately next to C constitute the level-1 cells (labeled “1”
in the figure), and the cells one or two cells away from C in any direction constitute
the level-2 cells (labeled “2” in the figure). The two levels of cells have the following
properties:
Level-1 cell property: Given any possible point, x of C, and any possible point, y, in
a level-1 cell, then dist(x, y) ≤ r.
Level-2 cell property: Given any possible point, x of C, and any point, y, such that
dist(x, y) ≥ r, then y is in a level-2 cell.

12.4 Proximity-Based Approaches

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

1

1

1

2

2

2

2

1

C

1

2

2

2

2

1

1

1

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

2

563

Figure 12.7 Grids in the CELL method.

Let a be the number of objects in cell C, b1 be the total number of objects in the
level-1 cells, and b2 be the total number of objects in the level-2 cells. We can apply the
following rules.
Level-1 cell pruning rule: Based on the level-1 cell property, if a + b1 > dπne, then
every object o in C is not a DB(r, π )-outlier because all those objects in C and
the level-1 cells are in the r-neighborhood of o, and there are at least dπne such
neighbors.
Level-2 cell pruning rule: Based on the level-2 cell property, if a + b1 + b2 <
dπne + 1, then all objects in C are DB(r, π )-outliers because each of their rneighborhoods has less than dπne other objects.
Using the preceding two rules, the CELL method organizes objects into groups using
a grid—all objects in a cell form a group. For groups satisfying one of the two rules, we
can determine that either all objects in a cell are outliers or nonoutliers, and thus do not
need to check those objects one by one. Moreover, to apply the two rules, we need only
check a limited number of cells close to a target cell instead of the whole data set.
Using the previous two rules, many objects can be determined as being either
nonoutliers or outliers. We only need to check the objects that cannot be pruned using
the two rules. Even for such an object, o, we need only compute the distance between
o and the objects in the level-2 cells with respect to o. This is because all objects in the
level-1 cells have a distance of at most r to o, and all objects not in a level-1 or level-2
cell must have a distance of more than r from o, and thus cannot be in the r-neighborhood of o.
When the data set is very large so that most of the data are stored on disk, the CELL
method may incur many random accesses to disk, which is costly. An alternative method
was proposed, which uses a very small amount of main memory (around 1% of the data

564

Chapter 12 Outlier Detection

set) to mine all outliers by scanning the data set three times. First, a sample, S, is created
of the given data set, D, using sampling by replacement. Each object in S is considered
the centroid of a partition. The objects in D are assigned to the partitions based on
distance. The preceding steps are completed in one scan of D. Candidate outliers are
identified in a second scan of D. After a third scan, all DB(r, π )-outliers have been found.

12.4.3 Density-Based Outlier Detection
Distance-based outliers, such as DB(r, π )-outliers, are just one type of outlier. Specifically, distance-based outlier detection takes a global view of the data set. Such outliers
can be regarded as “global outliers” for two reasons:
A DB(r, π)-outlier, for example, is far (as quantified by parameter r) from at least
(1 − π) × 100% of the objects in the data set. In other words, an outlier as such is
remote from the majority of the data.
To detect distance-based outliers, we need two global parameters, r and π, which are
applied to every outlier object.
Many real-world data sets demonstrate a more complex structure, where objects
may be considered outliers with respect to their local neighborhoods, rather than with
respect to the global data distribution. Let’s look at an example.
Example 12.14 Local proximity-based outliers. Consider the data points in Figure 12.8. There are two
clusters: C1 is dense, and C2 is sparse. Object o3 can be detected as a distance-based
outlier because it is far from the majority of the data set.
Now, let’s consider objects o1 and o2 . Are they outliers? On the one hand, the distance
from o1 and o2 to the objects in the dense cluster, C1 , is smaller than the average distance between an object in cluster C2 and its nearest neighbor. Thus, o1 and o2 are not
distance-based outliers. In fact, if we were to categorize o1 and o2 as DB(r, π )-outliers,
we would have to classify all the objects in clusters C2 as DB(r, π )-outliers.
On the other hand, o1 and o2 can be identified as outliers when they are considered
locally with respect to cluster C1 because o1 and o2 deviate significantly from the objects
in C1 . Moreover, o1 and o2 are also far from the objects in C2 .
C1

o1

o4

o2
C2
o3

Figure 12.8 Global outliers and local outliers.

12.4 Proximity-Based Approaches

565

To summarize, distance-based outlier detection methods cannot capture local outliers like o1 and o2 . Note that the distance between object o4 and its nearest neighbors is
much greater than the distance between o1 and its nearest neighbors. However, because
o4 is local to cluster C2 (which is sparse), o4 is not considered a local outlier.
“How can we formulate the local outliers as illustrated in Example 12.14?” The critical
idea here is that we need to compare the density around an object with the density
around its local neighbors. The basic assumption of density-based outlier detection
methods is that the density around a nonoutlier object is similar to the density around
its neighbors, while the density around an outlier object is significantly different from
the density around its neighbors.
Based on the preceding, density-based outlier detection methods use the relative density of an object against its neighbors to indicate the degree to which an object is an
outlier.
Now, let’s consider how to measure the relative density of an object, o, given a set of
objects, D. The k-distance of o, denoted by distk (o), is the distance, dist(o, p), between o
and another object, p ∈ D, such that
There are at least k objects o0 ∈ D−{o} such that dist(o, o0 ) ≤ dist(o, p).
There are at most k − 1 objects o00 ∈ D−{o} such that dist(o, o00 ) < dist(o, p).
In other words, distk (o) is the distance between o and its k-nearest neighbor. Consequently, the k-distance neighborhood of o contains all objects of which the distance to o
is not greater than distk (o), the k-distance of o, denoted by
Nk (o) = {o0 |o0 ∈ D, dist(o, o0 ) ≤ distk (o)}.

(12.11)

Note that Nk (o) may contain more than k objects because multiple objects may each be
the same distance away from o.
We can use the average distance from the objects in Nk (o) to o as the measure of the
local density of o. However, such a straightforward measure has a problem: If o has very
close neighbors o0 such that dist(o, o0 ) is very small, the statistical fluctuations of the
distance measure can be undesirably high. To overcome this problem, we can switch to
the following reachability distance measure by adding a smoothing effect.
For two objects, o and o0 , the reachability distance from o0 to o is dist(o ← o0 ) if dist
(o, o0 ) > distk (o), and distk (o) otherwise. That is,
reachdistk (o ← o0 ) = max{distk (o), dist(o, o0 )}.

(12.12)

Here, k is a user-specified parameter that controls the smoothing effect. Essentially, k
specifies the minimum neighborhood to be examined to determine the local density
of an object. Importantly, reachability distance is not symmetric, that is, in general,
reachdistk (o ← o0 ) 6= reachdistk (o0 ← o).

566

Chapter 12 Outlier Detection

Now, we can define the local reachability density of an object, o, as
kNk (o)k
.
lrdk (o) = P
0
0
o ∈Nk (o) reachdistk (o ← o)

(12.13)

There is a critical difference between the density measure here for outlier detection
and that in density-based clustering (Section 12.5). In density-based clustering, to determine whether an object can be considered a core object in a density-based cluster, we use
two parameters: a radius parameter, r, to specify the range of the neighborhood, and the
minimum number of points in the r-neighborhood. Both parameters are global and are
applied to every object. In contrast, as motivated by the observation that relative density
is the key to finding local outliers, we use the parameter k to quantify the neighborhood
and do not need to specify the minimum number of objects in the neighborhood as a
requirement of density. We instead calculate the local reachability density for an object
and compare it with that of its neighbors to quantify the degree to which the object is
considered an outlier.
Specifically, we define the local outlier factor of an object o as
P
lrdk (o0 )
o0 ∈Nk (o) lrdk (o)
=
LOFk (o) =
kNk (o)k

X
o0 ∈Nk (o)

lrdk (o0 ) ·

X

reachdistk (o0 ← o). (12.14)

o0 ∈Nk (o)

In other words, the local outlier factor is the average of the ratio of the local reachability
density of o and those of o’s k-nearest
P neighbors. The lower the local reachability density
of o (i.e., the smaller the item o0 ∈N (o) reachdistk (o0 ← o)) and the higher the local
k
reachability densities of the k-nearest neighbors of o, the higher the LOF value is. This
exactly captures a local outlier of which the local density is relatively low compared to
the local densities of its k-nearest neighbors.
The local outlier factor has some nice properties. First, for an object deep within a
consistent cluster, such as the points in the center of cluster C2 in Figure 12.8, the local
outlier factor is close to 1. This property ensures that objects inside clusters, no matter
whether the cluster is dense or sparse, will not be mislabeled as outliers.
Second, for an object o, the meaning of LOF(o) is easy to understand. Consider the
objects in Figure 12.9, for example. For object o, let
directmin (o) = min{reachdistk (o0 ← o)|o0 ∈ Nk (o)}

(12.15)

be the minimum reachability distance from o to its k-nearest neighbors. Similarly, we
can define
directmax (o) = max{reachdistk (o0 ← o)|o0 ∈ Nk (o)}.

(12.16)

We also consider the neighbors of o’s k-nearest neighbors. Let
indirectmin (o) = min{reachdistk (o00 ← o0 )|o0 ∈ Nk (o) and o00 ∈ Nk (o0 )}

(12.17)

12.5 Clustering-Based Approaches

567

C

k=3
indirectmax
directmin
o

directmax
indirectmin

Figure 12.9 A property of LOF(o).

and
indirectmax (o) = max{reachdistk (o00 ← o0 )|o0 ∈ Nk (o) and o00 ∈ Nk (o0 )}.

(12.18)

Then, it can be shown that LOF(o) is bounded as
directmin (o)
directmax (o)
≤ LOF(o) ≤
.
indirectmax (o)
indirectmin (o)

(12.19)

This result clearly shows that LOF captures the relative density of an object.

12.5

Clustering-Based Approaches
The notion of outliers is highly related to that of clusters. Clustering-based approaches
detect outliers by examining the relationship between objects and clusters. Intuitively,
an outlier is an object that belongs to a small and remote cluster, or does not belong to
any cluster.
This leads to three general approaches to clustering-based outlier detection. Consider
an object.
Does the object belong to any cluster? If not, then it is identified as an outlier.
Is there a large distance between the object and the cluster to which it is closest? If
yes, it is an outlier.
Is the object part of a small or sparse cluster? If yes, then all the objects in that cluster
are outliers.

568

Chapter 12 Outlier Detection

Let’s look at examples of each of these approaches.
Example 12.15 Detecting outliers as objects that do not belong to any cluster. Gregarious animals
(e.g., goats and deer) live and move in flocks. Using outlier detection, we can identify outliers as animals that are not part of a flock. Such animals may be either lost or
wounded.
In Figure 12.10, each point represents an animal living in a group. Using a densitybased clustering method, such as DBSCAN, we note that the black points belong to
clusters. The white point, a, does not belong to any cluster, and thus is declared an
outlier.
The second approach to clustering-based outlier detection considers the distance
between an object and the cluster to which it is closest. If the distance is large, then
the object is likely an outlier with respect to the cluster. Thus, this approach detects
individual outliers with respect to clusters.
Example 12.16 Clustering-based outlier detection using distance to the closest cluster. Using the
k-means clustering method, we can partition the data points shown in Figure 12.11 into
three clusters, as shown using different symbols. The center of each cluster is marked
with a +.
For each object, o, we can assign an outlier score to the object according to the distance between the object and the center that is closest to the object. Suppose the closest
center to o is co ; then the distance between o and co is dist(o, co ), and the average

a

Figure 12.10 Object a is an outlier because it does not belong to any cluster.
a

b

c

⫹ Cluster centers

Figure 12.11 Outliers (a, b, c) are far from the clusters to which they are closest (with respect to the cluster
centers).

12.5 Clustering-Based Approaches

distance between co and the objects assigned to o is lco . The ratio

569

dist(o,co )
lc o

measures how
dist(o, co ) stands out from the average. The larger the ratio, the farther away o is relative
from the center, and the more likely o is an outlier. In Figure 12.11, points a, b, and c
are relatively far away from their corresponding centers, and thus are suspected of being
outliers.

This approach can also be used for intrusion detection, as described in Example 12.17.
Example 12.17 Intrusion detection by clustering-based outlier detection. A bootstrap method was
developed to detect intrusions in TCP connection data by considering the similarity
between data points and the clusters in a training data set. The method consists of three
steps.
1. A training data set is used to find patterns of normal data. Specifically, the TCP connection data are segmented according to, say, dates. Frequent itemsets are found
in each segment. The frequent itemsets that are in a majority of the segments are
considered patterns of normal data and are referred to as “base connections.”
2. Connections in the training data that contain base connections are treated as attackfree. Such connections are clustered into groups.
3. The data points in the original data set are compared with the clusters mined in
step 2. Any point that is deemed an outlier with respect to the clusters is declared as
a possible attack.
Note that each of the approaches we have seen so far detects only individual objects
as outliers because they compare objects one at a time against clusters in the data set.
However, in a large data set, some outliers may be similar and form a small cluster. In
intrusion detection, for example, hackers who use similar tactics to attack a system may
form a cluster. The approaches discussed so far may be deceived by such outliers.
To overcome this problem, a third approach to cluster-based outlier detection identifies small or sparse clusters and declares the objects in those clusters to be outliers as well.
An example of this approach is the FindCBLOF algorithm, which works as follows.
1. Find clusters in a data set, and sort them according to decreasing size. The algorithm assumes that most of the data points are not outliers. It uses a parameter
α (0 ≤ α ≤ 1) to distinguish large from small clusters. Any cluster that contains at
least a percentage α (e.g., α = 90%) of the data set is considered a “large cluster.” The
remaining clusters are referred to as “small clusters.”
2. To each data point, assign a cluster-based local outlier factor (CBLOF). For a point
belonging to a large cluster, its CBLOF is the product of the cluster’s size and the
similarity between the point and the cluster. For a point belonging to a small cluster,
its CBLOF is calculated as the product of the size of the small cluster and the similarity
between the point and the closest large cluster.

570

Chapter 12 Outlier Detection

CBLOF defines the similarity between a point and a cluster in a statistical way that
represents the probability that the point belongs to the cluster. The larger the value, the
more similar the point and the cluster are. The CBLOF score can detect outlier points
that are far from any clusters. In addition, small clusters that are far from any large
cluster are considered to consist of outliers. The points with the lowest CBLOF scores
are suspected outliers.
Example 12.18 Detecting outliers in small clusters. The data points in Figure 12.12 form three clusters:
large clusters, C1 and C2 , and a small cluster, C3 . Object o does not belong to any cluster.
Using CBLOF, FindCBLOF can identify o as well as the points in cluster C3 as outliers.
For o, the closest large cluster is C1 . The CBLOF is simply the similarity between o and
C1 , which is small. For the points in C3 , the closest large cluster is C2 . Although there
are three points in cluster C3 , the similarity between those points and cluster C2 is low,
and |C3 | = 3 is small; thus, the CBLOF scores of points in C3 are small.
Clustering-based approaches may incur high computational costs if they have to find
clusters before detecting outliers. Several techniques have been developed for improved
efficiency. For example, fixed-width clustering is a linear-time technique that is used in
some outlier detection methods. The idea is simple yet efficient. A point is assigned to
a cluster if the center of the cluster is within a predefined distance threshold from the
point. If a point cannot be assigned to any existing cluster, a new cluster is created. The
distance threshold may be learned from the training data under certain conditions.
Clustering-based outlier detection methods have the following advantages. First, they
can detect outliers without requiring any labeled data, that is, in an unsupervised way.
They work for many data types. Clusters can be regarded as summaries of the data.
Once the clusters are obtained, clustering-based methods need only compare any object
against the clusters to determine whether the object is an outlier. This process is typically
fast because the number of clusters is usually small compared to the total number of
objects.

C3
C1

o

C2

Figure 12.12 Outliers in small clusters.

12.6 Classification-Based Approaches

571

A weakness of clustering-based outlier detection is its effectiveness, which depends
highly on the clustering method used. Such methods may not be optimized for outlier
detection. Clustering methods are often costly for large data sets, which can serve as a
bottleneck.

12.6

Classification-Based Approaches
Outlier detection can be treated as a classification problem if a training data set with class
labels is available. The general idea of classification-based outlier detection methods is
to train a classification model that can distinguish normal data from outliers.
Consider a training set that contains samples labeled as “normal” and others labeled
as “outlier.” A classifier can then be constructed based on the training set. Any classification method can be used (Chapters 8 and 9). This kind of brute-force approach,
however, does not work well for outlier detection because the training set is typically
heavily biased. That is, the number of normal samples likely far exceeds the number of
outlier samples. This imbalance, where the number of outlier samples may be insufficient, can prevent us from building an accurate classifier. Consider intrusion detection
in a system, for example. Because most system accesses are normal, it is easy to obtain
a good representation of the normal events. However, it is infeasible to enumerate all
potential intrusions, as new and unexpected attempts occur from time to time. Hence,
we are left with an insufficient representation of the outlier (or intrusion) samples.
To overcome this challenge, classification-based outlier detection methods often use a
one-class model. That is, a classifier is built to describe only the normal class. Any samples
that do not belong to the normal class are regarded as outliers.

Example 12.19 Outlier detection using a one-class model. Consider the training set shown in
Figure 12.13, where white points are samples labeled as “normal” and black points
are samples labeled as “outlier.” To build a model for outlier detection, we can learn
the decision boundary of the normal class using classification methods such as SVM
(Chapter 9), as illustrated. Given a new object, if the object is within the decision boundary of the normal class, it is treated as a normal case. If the object is outside the decision
boundary, it is declared an outlier.
An advantage of using only the model of the normal class to detect outliers is that
the model can detect new outliers that may not appear close to any outlier objects in the
training set. This occurs as long as such new outliers fall outside the decision boundary
of the normal class.
The idea of using the decision boundary of the normal class can be extended to
handle situations where the normal objects may belong to multiple classes such as in
fuzzy clustering (Chapter 11). For example, AllElectronics accepts returned items. Customers can return items for a number of reasons (corresponding to class categories)
such as “product design defects” and “product damaged during shipment.” Each such

572

Chapter 12 Outlier Detection

Figure 12.13 Learning a model for the normal class.
C1

C
a

Objects with label “normal”

Objects with label “outlier”

Objects without label

Figure 12.14 Detecting outliers by semi-supervised learning.

class is regarded as normal. To detect outlier cases, AllElectronics can learn a model for
each normal class. To determine whether a case is an outlier, we can run each model on
the case. If the case does not fit any of the models, then it is declared an outlier.
Classification-based methods and clustering-based methods can be combined to
detect outliers in a semi-supervised learning way.
Example 12.20 Outlier detection by semi-supervised learning. Consider Figure 12.14, where objects
are labeled as either “normal” or “outlier,” or have no label at all. Using a clusteringbased approach, we find a large cluster, C, and a small cluster, C1 . Because some objects
in C carry the label “normal,” we can treat all objects in this cluster (including those
without labels) as normal objects. We use the one-class model of this cluster to identify
normal objects in outlier detection. Similarly, because some objects in cluster C1 carry
the label “outlier,” we declare all objects in C1 as outliers. Any object that does not fall
into the model for C (e.g., a) is considered an outlier as well.

12.7 Mining Contextual and Collective Outliers

573

Classification-based methods can incorporate human domain knowledge into the
detection process by learning from the labeled samples. Once the classification model is
constructed, the outlier detection process is fast. It only needs to compare the objects
to be examined against the model learned from the training data. The quality of
classification-based methods heavily depends on the availability and quality of the training set. In many applications, it is difficult to obtain representative and high-quality
training data, which limits the applicability of classification-based methods.

12.7

Mining Contextual and Collective Outliers
An object in a given data set is a contextual outlier (or conditional outlier) if it deviates significantly with respect to a specific context of the object (Section 12.1). The
context is defined using contextual attributes. These depend heavily on the application, and are often provided by users as part of the contextual outlier detection task.
Contextual attributes can include spatial attributes, time, network locations, and sophisticated structured attributes. In addition, behavioral attributes define characteristics of
the object, and are used to evaluate whether the object is an outlier in the context to
which it belongs.

Example 12.21 Contextual outliers. To determine whether the temperature of a location is exceptional
(i.e., an outlier), the attributes specifying information about the location can serve as
contextual attributes. These attributes may be spatial attributes (e.g., longitude and latitude) or location attributes in a graph or network. The attribute time can also be used.
In customer-relationship management, whether a customer is an outlier may depend
on other customers with similar profiles. Here, the attributes defining customer profiles
provide the context for outlier detection.
In comparison to outlier detection in general, identifying contextual outliers requires
analyzing the corresponding contextual information. Contextual outlier detection
methods can be divided into two categories according to whether the contexts can be
clearly identified.

12.7.1 Transforming Contextual Outlier Detection
to Conventional Outlier Detection
This category of methods is for situations where the contexts can be clearly identified.
The idea is to transform the contextual outlier detection problem into a typical outlier
detection problem. Specifically, for a given data object, we can evaluate whether the
object is an outlier in two steps. In the first step, we identify the context of the object
using the contextual attributes. In the second step, we calculate the outlier score for the
object in the context using a conventional outlier detection method.

574

Chapter 12 Outlier Detection

Example 12.22 Contextual outlier detection when the context can be clearly identified. In customerrelationship management, we can detect outlier customers in the context of customer
groups. Suppose AllElectronics maintains customer information on four attributes,
namely age group (i.e., under 25, 25-45, 45-65, and over 65), postal code, number of
transactions per year, and annual total transaction amount. The attributes age group
and postal code serve as contextual attributes, and the attributes number of
transactions per year and annual total transaction amount are behavioral attributes.
To detect contextual outliers in this setting, for a customer, c, we can first locate the
context of c using the attributes age group and postal code. We can then compare c with
the other customers in the same group, and use a conventional outlier detection method,
such as some of the ones discussed earlier, to determine whether c is an outlier.
Contexts may be specified at different levels of granularity. Suppose AllElectronics
maintains customer information at a more detailed level for the attributes age,
postal code, number of transactions per year, and annual total transaction amount. We
can still group customers on age and postal code, and then mine outliers in each group.
What if the number of customers falling into a group is very small or even zero? For a
customer, c, if the corresponding context contains very few or even no other customers,
the evaluation of whether c is an outlier using the exact context is unreliable or even
impossible.
To overcome this challenge, we can assume that customers of similar age and who
live within the same area should have similar normal behavior. This assumption can
help to generalize contexts and makes for more effective outlier detection. For example,
using a set of training data, we may learn a mixture model, U , of the data on the contextual attributes, and another mixture model, V , of the data on the behavior attributes.
A mapping p(Vi |Uj ) is also learned to capture the probability that a data object o belonging to cluster Uj on the contextual attributes is generated by cluster Vi on the behavior
attributes. The outlier score can then be calculated as
S(o) =

X
Uj

p(o ∈ Uj )

X

p(o ∈ Vi )p(Vi |Uj ).

(12.20)

Vi

Thus, the contextual outlier problem is transformed into outlier detection using mixture models.

12.7.2 Modeling Normal Behavior with Respect to Contexts
In some applications, it is inconvenient or infeasible to clearly partition the data into
contexts. For example, consider the situation where the online store of AllElectronics
records customer browsing behavior in a search log. For each customer, the data log contains the sequence of products searched for and browsed by the customer. AllElectronics
is interested in contextual outlier behavior, such as if a customer suddenly purchased a
product that is unrelated to those she recently browsed. However, in this application,
contexts cannot be easily specified because it is unclear how many products browsed

12.7 Mining Contextual and Collective Outliers

575

earlier should be considered as the context, and this number will likely differ for each
product.
This second category of contextual outlier detection methods models the normal
behavior with respect to contexts. Using a training data set, such a method trains a
model that predicts the expected behavior attribute values with respect to the contextual
attribute values. To determine whether a data object is a contextual outlier, we can then
apply the model to the contextual attributes of the object. If the behavior attribute values of the object significantly deviate from the values predicted by the model, then the
object can be declared a contextual outlier.
By using a prediction model that links the contexts and behavior, these methods
avoid the explicit identification of specific contexts. A number of classification and
prediction techniques can be used to build such models such as regression, Markov
models, and finite state automaton. Interested readers are referred to Chapters 8 and
9 on classification and the bibliographic notes for further details (Section 12.11).
In summary, contextual outlier detection enhances conventional outlier detection
by considering contexts, which are important in many applications. We may be able
to detect outliers that cannot be detected otherwise. Consider a credit card user
whose income level is low but whose expenditure patterns are similar to those of
millionaires. This user can be detected as a contextual outlier if the income level
is used to define context. Such a user may not be detected as an outlier without
contextual information because she does share expenditure patterns with many millionaires. Considering contexts in outlier detection can also help to avoid false alarms.
Without considering the context, a millionaire’s purchase transaction may be falsely
detected as an outlier if the majority of customers in the training set are not millionaires. This can be corrected by incorporating contextual information in outlier
detection.

12.7.3 Mining Collective Outliers
A group of data objects forms a collective outlier if the objects as a whole deviate significantly from the entire data set, even though each individual object in the group may
not be an outlier (Section 12.1). To detect collective outliers, we have to examine the
structure of the data set, that is, the relationships between multiple data objects. This
makes the problem more difficult than conventional and contextual outlier detection.
“How can we explore the data set structure?” This typically depends on the nature
of the data. For outlier detection in temporal data (e.g., time series and sequences), we
explore the structures formed by time, which occur in segments of the time series or subsequences. To detect collective outliers in spatial data, we explore local areas. Similarly,
in graph and network data, we explore subgraphs. Each of these structures is inherent to
its respective data type.
Contextual outlier detection and collective outlier detection are similar in that they
both explore structures. In contextual outlier detection, the structures are the contexts,
as specified by the contextual attributes explicitly. The critical difference in collective
outlier detection is that the structures are often not explicitly defined, and have to be
discovered as part of the outlier detection process.

576

Chapter 12 Outlier Detection

As with contextual outlier detection, collective outlier detection methods can also be
divided into two categories. The first category consists of methods that reduce the problem to conventional outlier detection. Its strategy is to identify structure units, treat each
structure unit (e.g., a subsequence, a time-series segment, a local area, or a subgraph)
as a data object, and extract features. The problem of collective outlier detection is thus
transformed into outlier detection on the set of “structured objects” constructed as such
using the extracted features. A structure unit, which represents a group of objects in the
original data set, is a collective outlier if the structure unit deviates significantly from the
expected trend in the space of the extracted features.
Example 12.23 Collective outlier detection on graph data. Let’s see how we can detect collective outliers in AllElectronics’ online social network of customers. Suppose we treat the social
network as an unlabeled graph. We then treat each possible subgraph of the network as
a structure unit. For each subgraph, S, let |S| be the number of vertices in S, and freq(S)
be the frequency of S in the network. That is, freq(S) is the number of different subgraphs
in the network that are isomorphic to S. We can use these two features to detect outlier
subgraphs. An outlier subgraph is a collective outlier that contains multiple vertices.
In general, a small subgraph (e.g., a single vertex or a pair of vertices connected by
an edge) is expected to be frequent, and a large subgraph is expected to be infrequent.
Using the preceding simple method, we can detect small subgraphs that are of very low
frequency or large subgraphs that are surprisingly frequent. These are outlier structures
in the social network.
Predefining the structure units for collective outlier detection can be difficult or
impossible. Consequently, the second category of methods models the expected behavior of structure units directly. For example, to detect collective outliers in temporal
sequences, one method is to learn a Markov model from the sequences. A subsequence
can then be declared as a collective outlier if it significantly deviates from the model.
In summary, collective outlier detection is subtle due to the challenge of exploring the structures in data. The exploration typically uses heuristics, and thus may be
application-dependent. The computational cost is often high due to the sophisticated
mining process. While highly useful in practice, collective outlier detection remains a
challenging direction that calls for further research and development.

12.8

Outlier Detection in High-Dimensional Data
In some applications, we may need to detect outliers in high-dimensional data. The
dimensionality curse poses huge challenges for effective outlier detection. As the dimensionality increases, the distance between objects may be heavily dominated by noise.
That is, the distance and similarity between two points in a high-dimensional space
may not reflect the real relationship between the points. Consequently, conventional
outlier detection methods, which mainly use proximity or density to identify outliers,
deteriorate as dimensionality increases.

12.8 Outlier Detection in High-Dimensional Data

577

Ideally, outlier detection methods for high-dimensional data should meet the challenges that follow.
Interpretation of outliers: They should be able to not only detect outliers, but also
provide an interpretation of the outliers. Because many features (or dimensions) are
involved in a high-dimensional data set, detecting outliers without providing any
interpretation as to why they are outliers is not very useful. The interpretation of
outliers may come from, for example, specific subspaces that manifest the outliers
or an assessment regarding the “outlier-ness” of the objects. Such interpretation can
help users to understand the possible meaning and significance of the outliers.
Data sparsity: The methods should be capable of handling sparsity in highdimensional spaces. The distance between objects becomes heavily dominated by
noise as the dimensionality increases. Therefore, data in high-dimensional spaces are
often sparse.
Data subspaces: They should model outliers appropriately, for example, adaptive
to the subspaces signifying the outliers and capturing the local behavior of data.
Using a fixed-distance threshold against all subspaces to detect outliers is not a
good idea because the distance between two objects monotonically increases as the
dimensionality increases.
Scalability with respect to dimensionality: As the dimensionality increases, the
number of subspaces increases exponentially. An exhaustive combinatorial exploration of the search space, which contains all possible subspaces, is not a scalable
choice.
Outlier detection methods for high-dimensional data can be divided into three main
approaches. These include extending conventional outlier detection (Section 12.8.1),
finding outliers in subspaces (Section 12.8.2), and modeling high-dimensional outliers
(Section 12.8.3).

12.8.1 Extending Conventional Outlier Detection
One approach for outlier detection in high-dimensional data extends conventional outlier detection methods. It uses the conventional proximity-based models of outliers.
However, to overcome the deterioration of proximity measures in high-dimensional
spaces, it uses alternative measures or constructs subspaces and detects outliers there.
The HilOut algorithm is an example of this approach. HilOut finds distance-based
outliers, but uses the ranks of distance instead of the absolute distance in outlier detection. Specifically, for each object, o, HilOut finds the k-nearest neighbors of o, denoted
by nn1 (o), . . . , nnk (o), where k is an application-dependent parameter. The weight of
object o is defined as

w(o) =

k
X
i=1

dist(o, nni (o)).

(12.21)

578

Chapter 12 Outlier Detection

All objects are ranked in weight-descending order. The top-l objects in weight are output
as outliers, where l is another user-specified parameter.
Computing the k-nearest neighbors for every object is costly and does not scale up
when the dimensionality is high and the database is large. To address the scalability issue,
HilOut employs space-filling curves to achieve an approximation algorithm, which is
scalable in both running time and space with respect to database size and dimensionality.
While some methods like HilOut detect outliers in the full space despite the high
dimensionality, other methods reduce the high-dimensional outlier detection problem to a lower-dimensional one by dimensionality reduction (Chapter 3). The idea
is to reduce the high-dimensional space to a lower-dimensional space where normal
instances can still be distinguished from outliers. If such a lower-dimensional space can
be found, then conventional outlier detection methods can be applied.
To reduce dimensionality, general feature selection and extraction methods may be
used or extended for outlier detection. For example, principal components analysis
(PCA) can be used to extract a lower-dimensional space. Heuristically, the principal
components with low variance are preferred because, on such dimensions, normal
objects are likely close to each other and outliers often deviate from the majority.
By extending conventional outlier detection methods, we can reuse much of the experience gained from research in the field. These new methods, however, are limited. First,
they cannot detect outliers with respect to subspaces and thus have limited interpretability. Second, dimensionality reduction is feasible only if there exists a lower-dimensional
space where normal objects and outliers are well separated. This assumption may not
hold true.

12.8.2 Finding Outliers in Subspaces
Another approach for outlier detection in high-dimensional data is to search for outliers
in various subspaces. A unique advantage is that, if an object is found to be an outlier
in a subspace of much lower dimensionality, the subspace provides critical information
for interpreting why and to what extent the object is an outlier. This insight is highly
valuable in applications with high-dimensional data due to the overwhelming number
of dimensions.
Example 12.24 Outliers in subspaces. As a customer-relationship manager at AllElectronics, you are
interested in finding outlier customers. AllElectronics maintains an extensive customer
information database, which contains many attributes and the transaction history of
customers. The database is high dimensional.
Suppose you find that a customer, Alice, is an outlier in a lower-dimensional subspace that contains the dimensions average transaction amount and purchase frequency,
such that her average transaction amount is substantially larger than the majority of
the customers, and her purchase frequency is dramatically lower. The subspace itself
speaks for why and to what extent Alice is an outlier. Using this information, you strategically decide to approach Alice by suggesting options that could improve her purchase
frequency at AllElectronics.

12.8 Outlier Detection in High-Dimensional Data

579

“How can we detect outliers in subspaces?” We use a grid-based subspace outlier
detection method to illustrate. The major ideas are as follows. We consider projections of
the data onto various subspaces. If, in a subspace, we find an area that has a density that
is much lower than average, then the area may contain outliers. To find such projections,
we first discretize the data into a grid in an equal-depth way. That is, each dimension is
partitioned
 into φ equal-depth ranges, where each range contains a fraction, f , of the
objects f = φ1 . Equal-depth partitioning is used because data along different dimensions may have different localities. An equal-width partitioning of the space may not be
able to reflect such differences in locality.
Next, we search for regions defined by ranges in subspaces that are significantly sparse. To quantify what we mean by “significantly sparse,” let’s consider a
k-dimensional cube formed by k ranges on k dimensions. Suppose the data set contains n objects. If the objects are independently distributed, the expected number of
 k
objects falling into a k-dimensional region is φ1 n = f k n. The standard deviation of
p
the number of points in a k-dimensional region is f k (1 − f k )n. Suppose a specific
k-dimensional cube C has n(C) objects. We can define the sparsity coefficient of C as
n(C) − f k n
S(C) = p
.
f k (1 − f k )n

(12.22)

If S(C) < 0, then C contains fewer objects than expected. The smaller the value of S(C)
(i.e., the more negative), the sparser C is and the more likely the objects in C are outliers
in the subspace.
By assuming S(C) follows a normal distribution, we can use normal distribution
tables to determine the probabilistic significance level for an object that deviates dramatically from the average for an a priori assumption of the data following a uniform
distribution. In general, the assumption of uniform distribution does not hold. However, the sparsity coefficient still provides an intuitive measure of the “outlier-ness” of a
region.
To find cubes of significantly small sparsity coefficient values, a brute-force approach
is to search every cube in every possible subspace. The cost of this, however, is
immediately exponential. An evolutionary search can be conducted, which improves efficiency at the expense of accuracy. For details, please refer to the bibliographic notes
(Section 12.11). The objects contained by cubes of very small sparsity coefficient values
are output as outliers.
In summary, searching for outliers in subspaces is advantageous in that the outliers
found tend to be better understood, owing to the context provided by the subspaces.
Challenges include making the search efficient and scalable.

12.8.3 Modeling High-Dimensional Outliers
An alternative approach for outlier detection methods in high-dimensional data tries to
develop new models for high-dimensional outliers directly. Such models typically avoid

580

Chapter 12 Outlier Detection

proximity measures and instead adopt new heuristics to detect outliers, which do not
deteriorate in high-dimensional data.
Let’s examine angle-based outlier detection (ABOD) as an example.
Example 12.25 Angle-based outliers. Figure 12.15 contains a set of points forming a cluster, with the
exception of c, which is an outlier. For each point o, we examine the angle ∠xoy for every
pair of points x, y such that x 6= o, y 6= o. The figure shows angle ∠dae as an example.
Note that for a point in the center of a cluster (e.g., a), the angles formed as such
differ widely. For a point that is at the border of a cluster (e.g., b), the angle variation is
smaller. For a point that is an outlier (e.g., c), the angle variable is substantially smaller.
This observation suggests that we can use the variance of angles for a point to determine
whether a point is an outlier.
We can combine angles and distance to model outliers. Mathematically, for each
point o, we use the distance-weighted angle variance as the outlier score. That is, given a
set of points, D, for a point, o ∈ D, we define the angle-based outlier factor (ABOF) as
ABOF(o) = VARx ,y ∈D,x 6=o,y 6=o

→
→
h−
ox, −
oyi
,
dist(o, x)2 dist(o, y)2

(12.23)

where h, i is the scalar product operator, and dist(, ) is a norm distance.
Clearly, the farther away a point is from clusters and the smaller the variance of the
angles of a point, the smaller the ABOF. The ABOD computes the ABOF for each point,
and outputs a list of the points in the data set in ABOF-ascending order.
Computing the exact ABOF for every point in a database is costly, requiring a time
complexity of O(n3 ), where n is the number of points in the database. Obviously, this
exact algorithm does not scale up for large data sets. Approximation methods have been
developed to speed up the computation. The angle-based outlier detection idea has been
generalized to handle arbitrary data types. For additional details, see the bibliographic
notes (Section 12.11).
Developing native models for high-dimensional outliers can lead to effective methods. However, finding good heuristics for detecting high-dimensional outliers is difficult. Efficiency and scalability on large and high-dimensional data sets are major
challenges.
c

d
a
b

e

Figure 12.15 Angle-based outliers.

12.9 Summary

12.9

581

Summary
Assume that a given statistical process is used to generate a set of data objects. An
outlier is a data object that deviates significantly from the rest of the objects, as if it
were generated by a different mechanism.
Types of outliers include global outliers, contextual outliers, and collective outliers.
An object may be more than one type of outlier.
Global outliers are the simplest form of outlier and the easiest to detect. A contextual
outlier deviates significantly with respect to a specific context of the object (e.g., a
Toronto temperature value of 28◦ C is an outlier if it occurs in the context of winter).
A subset of data objects forms a collective outlier if the objects as a whole deviate
significantly from the entire data set, even though the individual data objects may not
be outliers. Collective outlier detection requires background information to model
the relationships among objects to find outlier groups.
Challenges in outlier detection include finding appropriate data models, the dependence of outlier detection systems on the application involved, finding ways to
distinguish outliers from noise, and providing justification for identifying outliers
as such.
Outlier detection methods can be categorized according to whether the sample
of data for analysis is given with expert-provided labels that can be used to build
an outlier detection model. In this case, the detection methods are supervised,
semi-supervised, or unsupervised. Alternatively, outlier detection methods may be
organized according to their assumptions regarding normal objects versus outliers. This categorization includes statistical methods, proximity-based methods, and
clustering-based methods.
Statistical outlier detection methods (or model-based methods) assume that the
normal data objects follow a statistical model, where data not following the model
are considered outliers. Such methods may be parametric (they assume that the data
are generated by a parametric distribution) or nonparametric (they learn a model for
the data, rather than assuming one a priori). Parametric methods for multivariate
data may employ the Mahalanobis distance, the χ 2 -statistic, or a mixture of multiple parametric models. Histograms and kernel density estimation are examples of
nonparametric methods.
Proximity-based outlier detection methods assume that an object is an outlier
if the proximity of the object to its nearest neighbors significantly deviates from
the proximity of most of the other objects to their neighbors in the same data
set. Distance-based outlier detection methods consult the neighborhood of an object,
defined by a given radius. An object is an outlier if its neighborhood does not have
enough other points. In density-based outlier detection methods, an object is an outlier
if its density is relatively much lower than that of its neighbors.

582

Chapter 12 Outlier Detection

Clustering-based outlier detection methods assume that the normal data objects
belong to large and dense clusters, whereas outliers belong to small or sparse clusters,
or do not belong to any clusters.
Classification-based outlier detection methods often use a one-class model. That is,
a classifier is built to describe only the normal class. Any samples that do not belong
to the normal class are regarded as outliers.
Contextual outlier detection and collective outlier detection explore structures in
the data. In contextual outlier detection, the structures are defined as contexts using
contextual attributes. In collective outlier detection, the structures are implicit and
are explored as part of the mining process. To detect such outliers, one approach
transforms the problem into one of conventional outlier detection. Another
approach models the structures directly.
Outlier detection methods for high-dimensional data can be divided into three
main approaches. These include extending conventional outlier detection, finding
outliers in subspaces, and modeling high-dimensional outliers.

12.10

Exercises

12.1 Give an application example where global outliers, contextual outliers, and collective
outliers are all interesting. What are the attributes, and what are the contextual and
behavioral attributes? How is the relationship among objects modeled in collective
outlier detection?
12.2 Give an application example of where the border between normal objects and outliers is
often unclear, so that the degree to which an object is an outlier has to be well estimated.
12.3 Adapt a simple semi-supervised method for outlier detection. Discuss the scenario
where you have (a) only some labeled examples of normal objects, and (b) only some
labeled examples of outliers.
12.4 Using an equal-depth histogram, design a way to assign an object an outlier score.
12.5 Consider the nested loop approach to mining distance-based outliers (Figure 12.6). Suppose the objects in a data set are arranged randomly, that is, each object has the same
probability to appear in a position. Show that when the number of outlier objects is
small with respect to the total number of objects in the whole data set, the expected
number of distance calculations is linear to the number of objects.
12.6 In the density-based outlier detection method of Section 12.4.3, the definition of local
reachability density has a potential problem: lrdk (o) = ∞ may occur. Explain why this
may occur and propose a fix to the issue.
12.7 Because clusters may form a hierarchy, outliers may belong to different granularity
levels. Propose a clustering-based outlier detection method that can find outliers at
different levels.

12.11 Bibliographic Notes

583

12.8 In outlier detection by semi-supervised learning, what is the advantage of using objects
without labels in the training data set?
12.9 To understand why angle-based outlier detection is a heuristic method, give an example
where it does not work well. Can you come up with a method to overcome this issue?

12.11

Bibliographic Notes
Hawkins [Haw80] defined outliers from a statistics angle. For surveys or tutorials on
the subject of outlier and anomaly detection, see Chandola, Banerjee, and Kumar
[CBK09]; Hodge and Austin [HA04]; Agyemang, Barker, and Alhajj [ABA06]; Markou
and Singh [MS03a, MS03b]; Patcha and Park [PP07]; Beckman and Cook [BC83]; BenGal [B-G05]; and Bakar, Mohemad, Ahmad, and Deris [BMAD06]. Song, Wu, Jermaine,
et al. [SWJR-07] proposed the notion of conditional anomaly and contextual outlier
detection.
Fujimaki, Yairi, and Machida [FYM05] presented an example of semi-supervised outlier detection using a set of labeled “normal” objects. For an example of semi-supervised
outlier detection using labeled outliers, see Dasgupta and Majumdar [DM02].
Shewhart [She31] assumed that most objects follow a Gaussian distribution and
used 3σ as the threshold for identifying outliers, where σ is the standard deviation. Boxplots are used to detect and visualize outliers in various applications such
as medical data (Horn, Feng, Li, and Pesce [HFLP01]). Grubb’s test was described by
Grubbs [Gru69], Stefansky [Ste72], and Anscombe and Guttman [AG60]. Laurikkala,
Juhola, and Kentala [LJK00] and Aggarwal and Yu [AY01] extended Grubb’s test to
detect multivariate outliers. Use of the χ 2 -statistic to detect multivariate outliers was
studied by Ye and Chen [YC01].
Agarwal [Aga06] used Gaussian mixture models to capture “normal data.” Abraham
and Box [AB79] assumed that outliers are generated by a normal distribution with a
substantially larger variance. Eskin [Esk00] used the EM algorithm to learn mixture
models for normal data and outliers.
Histogram-based outlier detection methods are popular in the application domain
of intrusion detection (Eskin [Esk00] and Eskin, Arnold, Prerau, et al. [EAP+ 02]) and
fault detection (Fawcett and Provost [FP97]).
The notion of distance-based outliers was developed by Knorr and Ng [KN97]. The
index-based, nested loop–based, and grid-based approaches were explored (Knorr and
Ng [KN98] and Knorr, Ng, and Tucakov [KNT00]) to speed up distance-based outlier detection. Bay and Schwabacher [BS03] and Jin, Tung, and Han [JTH01] pointed
out that the CPU runtime of the nested loop method is often scalable with respect
to database size. Tao, Xiao, and Zhou [TXZ06] presented an algorithm that finds all
distance-based outliers by scanning the database three times with fixed main memory.
For larger memory size, they proposed a method that uses only one or two scans.
The notion of density-based outliers was first developed by Breunig, Kriegel, Ng,
and Sander [BKNS00]. Various methods proposed with the theme of density-based

584

Chapter 12 Outlier Detection

outlier detection include Jin, Tung, and Han [JTH01]; Jin, Tung, Han, and Wang
[JTHW06]; and Papadimitriou, Kitagawa, Gibbons, et al. [PKG-F03]. The variations
differ in how they estimate density.
The bootstrap method discussed in Example 12.17 was developed by Barbara,
Li, Couto, et al. [BLC+ 03]. The FindCBOLF algorithm was given by He, Xu, and
Deng [HXD03]. For the use of fixed-width clustering in outlier detection methods, see Eskin, Arnold, and Prerau, et al. [EAP+ 02]; Mahoney and Chan [MC03];
and He, Xu, and Deng [HXD03]. Barbara, Wu, and Jajodia [BWJ01] used multiclass
classification in network intrusion detection.
Song, Wu, Jermaine, et al. [SWJR07] and Fawcet and Provost [FP97] presented a
method to reduce the problem of contextual outlier detection to one of conventional
outlier detection. Yi, Sidiropoulos, Johnson, Jagadish, et al. [YSJJ+ 00] used regression techniques to detect contextual outliers in co-evolving sequences. The idea in
Example 12.22 for collective outlier detection on graph data is based on Noble and Cook
[NC03].
The HilOut algorithm was proposed by Angiulli and Pizzuti [AP05]. Aggarwal and
Yu [AY01] developed the sparsity coefficient–based subspace outlier detection method.
Kriegel, Schubert, and Zimek [KSZ08] proposed angle-based outlier detection.

13

Data Mining Trends
and Research Frontiers
As a young research field, data mining has made significant progress and covered a broad spectrum of applications since the 1980s. Today, data mining is used in a vast array of
areas. Numerous commercial data mining systems and services are available. Many challenges, however, still remain. In this final chapter, we introduce the mining of complex
data types as a prelude to further in-depth study readers may choose to do. In addition, we focus on trends and research frontiers in data mining. Section 13.1 presents an
overview of methodologies for mining complex data types, which extend the concepts
and tasks introduced in this book. Such mining includes mining time-series, sequential
patterns, and biological sequences; graphs and networks; spatiotemporal data, including
geospatial data, moving-object data, and cyber-physical system data; multimedia data;
text data; web data; and data streams. Section 13.2 briefly introduces other approaches
to data mining, including statistical methods, theoretical foundations, and visual and
audio data mining.
In Section 13.3, you will learn more about data mining applications in business and
in science, including the financial retail, and telecommunication industries, science and
engineering, and recommender systems. The social impacts of data mining are discussed
in Section 13.4, including ubiquitous and invisible data mining, and privacy-preserving
data mining. Finally, in Section 13.5 we speculate on current and expected data mining
trends that arise in response to new challenges in the field.

13.1

Mining Complex Data Types
In this section, we outline the major developments and research efforts in mining complex data types. Complex data types are summarized in Figure 13.1. Section 13.1.1
covers mining sequence data such as time-series, symbolic sequences, and biological
sequences. Section 13.1.2 discusses mining graphs and social and information networks.
Section 13.1.3 addresses mining other kinds of data, including spatial data, spatiotemporal data, moving-object data, cyber-physical system data, multimedia data, text data,

Data Mining: Concepts and Techniques
c 2012 Elsevier Inc. All rights reserved.

585

586

