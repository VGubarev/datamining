Chapter 13 Data Mining Trends and Research Frontiers

C
o
m
p
l
e
x
T
y
p
e
s
o
f
D
a
t
a

Sequence
Data

Graphs and
Network Mining

Other Kinds
of Data

Time-series data (e.g., stock market data)
Symbolic sequences (e.g., customer shopping
sequences, web click streams)
Biological sequences (e.g., DNA and protein
sequences)
Homogeneous (nodes/links are of same type)
or heterogeneous (nodes/links are of different
types)
Examples: Graphs and social, and information
networks, etc.

Spatial data
Spatiotemporal data
Cyber-physical system data
Multimedia data
Text data
Web data
Data streams

Figure 13.1 Complex data types for mining.

web data, and data streams. Due to the broad scope of these themes, this section presents
only a high-level overview; these topics are not discussed in-depth in this book.

13.1.1 Mining Sequence Data: Time-Series, Symbolic
Sequences, and Biological Sequences
A sequence is an ordered list of events. Sequences may be categorized into three groups,
based on the characteristics of the events they describe: (1) time-series data, (2) symbolic
sequence data, and (3) biological sequences. Let’s consider each type.
In time-series data, sequence data consist of long sequences of numeric data,
recorded at equal time intervals (e.g., per minute, per hour, or per day). Time-series
data can be generated by many natural and economic processes such as stock markets,
and scientific, medical, or natural observations.
Symbolic sequence data consist of long sequences of event or nominal data, which
typically are not observed at equal time intervals. For many such sequences, gaps (i.e.,
lapses between recorded events) do not matter much. Examples include customer shopping sequences and web click streams, as well as sequences of events in science and
engineering and in natural and social developments.
Biological sequences include DNA and protein sequences. Such sequences are typically very long, and carry important, complicated, but hidden semantic meaning. Here,
gaps are usually important.
Let’s look into data mining for each of these sequence data types.

13.1 Mining Complex Data Types

587

Similarity Search in Time-Series Data
A time-series data set consists of sequences of numeric values obtained over repeated
measurements of time. The values are typically measured at equal time intervals (e.g.,
every minute, hour, or day). Time-series databases are popular in many applications
such as stock market analysis, economic and sales forecasting, budgetary analysis, utility studies, inventory studies, yield projections, workload projections, and process and
quality control. They are also useful for studying natural phenomena (e.g., atmosphere,
temperature, wind, earthquake), scientific and engineering experiments, and medical
treatments.
Unlike normal database queries, which find data that match a given query exactly,
a similarity search finds data sequences that differ only slightly from the given query
sequence. Many time-series similarity queries require subsequence matching, that is,
finding a set of sequences that contain subsequences that are similar to a given query
sequence.
For similarity search, it is often necessary to first perform data or dimensionality
reduction and transformation of time-series data. Typical dimensionality reduction techniques include (1) the discrete Fourier transform (DFT), (2) discrete wavelet transforms
(DWT), and (3) singular value decomposition (SVD) based on principle components analysis (PCA). Because we touched on these concepts in Chapter 3, and because a thorough
explanation is beyond the scope of this book, we will not go into great detail here. With
such techniques, the data or signal is mapped to a signal in a transformed space. A small
subset of the “strongest” transformed coefficients are saved as features.
These features form a feature space, which is a projection of the transformed space.
Indices can be constructed on the original or transformed time-series data to speed
up a search. For a query-based similarity search, techniques include normalization
transformation, atomic matching (i.e., finding pairs of gap-free windows of a small
length that are similar), window stitching (i.e., stitching similar windows to form pairs
of large similar subsequences, allowing gaps between atomic matches), and subsequence ordering (i.e., linearly ordering the subsequence matches to determine whether
enough similar pieces exist). Numerous software packages exist for a similarity search in
time-series data.
Recently, researchers have proposed transforming time-series data into piecewise
aggregate approximations so that the data can be viewed as a sequence of symbolic representations. The problem of similarity search is then transformed into one of matching
subsequences in symbolic sequence data. We can identify motifs (i.e., frequently occurring sequential patterns) and build index or hashing mechanisms for an efficient search
based on such motifs. Experiments show this approach is fast and simple, and has
comparable search quality to that of DFT, DWT, and other dimensionality reduction
methods.

Regression and Trend Analysis in Time-Series Data
Regression analysis of time-series data has been studied substantially in the fields of
statistics and signal analysis. However, one may often need to go beyond pure regression

Chapter 13 Data Mining Trends and Research Frontiers

Price

588

AllElectronics stock
10-day moving average

Time

Figure 13.2 Time-series data for the stock price of AllElectronics over time. The trend is shown with a
dashed curve, calculated by a moving average.

analysis and perform trend analysis for many practical applications. Trend analysis
builds an integrated model using the following four major components or movements
to characterize time-series data:
1. Trend or long-term movements: These indicate the general direction in which a
time-series graph is moving over time, for example, using weighted moving average
and the least squares methods to find trend curves such as the dashed curve indicated
in Figure 13.2.
2. Cyclic movements: These are the long-term oscillations about a trend line or curve.
3. Seasonal variations: These are nearly identical patterns that a time series appears
to follow during corresponding seasons of successive years such as holiday shopping
seasons. For effective trend analysis, the data often need to be “deseasonalized” based
on a seasonal index computed by autocorrelation.
4. Random movements: These characterize sporadic changes due to chance events such
as labor disputes or announced personnel changes within companies.
Trend analysis can also be used for time-series forecasting, that is, finding a mathematical function that will approximately generate the historic patterns in a time
series, and using it to make long-term or short-term predictions of future values.
ARIMA (auto-regressive integrated moving average), long-memory time-series modeling,
and autoregression are popular methods for such analysis.

Sequential Pattern Mining in Symbolic Sequences
A symbolic sequence consists of an ordered set of elements or events, recorded with
or without a concrete notion of time. There are many applications involving data of

13.1 Mining Complex Data Types

589

symbolic sequences such as customer shopping sequences, web click streams, program
execution sequences, biological sequences, and sequences of events in science and
engineering and in natural and social developments. Because biological sequences carry
very complicated semantic meaning and pose many challenging research issues, most
investigations are conducted in the field of bioinformatics.
Sequential pattern mining has focused extensively on mining symbolic sequences.
A sequential pattern is a frequent subsequence existing in a single sequence or a
set of sequences. A sequence α = ha1 a2 · · · an i is a subsequence of another sequence
β = hb1 b2 · · · bm i if there exist integers 1 ≤ j1 < j2 < · · · < jn ≤ m such that a1 ⊆ bj1 ,
a2 ⊆ bj2 , . . . , an ⊆ bjn . For example, if α = h{ab},di and β = h{abc}, {be}, {de}, ai, where
a, b, c, d, and e are items, then α is a subsequence of β. Mining of sequential patterns
consists of mining the set of subsequences that are frequent in one sequence or a set of
sequences. Many scalable algorithms have been developed as a result of extensive studies in this area. Alternatively, we can mine only the set of closed sequential patterns,
where a sequential pattern s is closed if there exists no sequential pattern s 0 , where s
is a proper subsequence of s 0 , and s 0 has the same (frequency) support as s. Similar to
its frequent pattern mining counterpart, there are also studies on efficient mining of
multidimensional, multilevel sequential patterns.
As with constraint-based frequent pattern mining, user-specified constraints can be
used to reduce the search space in sequential pattern mining and derive only the patterns
that are of interest to the user. This is referred to as constraint-based sequential pattern
mining. Moreover, we may relax constraints or enforce additional constraints on the
problem of sequential pattern mining to derive different kinds of patterns from sequence
data. For example, we can enforce gap constraints so that the patterns derived contain only consecutive subsequences or subsequences with very small gaps. Alternatively,
we may derive periodic sequential patterns by folding events into proper-size windows
and finding recurring subsequences in these windows. Another approach derives partial
order patterns by relaxing the requirement of strict sequential ordering in the mining of
subsequence patterns. Besides mining partial order patterns, sequential pattern mining
methodology can also be extended to mining trees, lattices, episodes, and some other
ordered patterns.

Sequence Classification
Most classification methods perform model construction based on feature vectors.
However, sequences do not have explicit features. Even with sophisticated feature selection techniques, the dimensionality of potential features can still be very high and the
sequential nature of features is difficult to capture. This makes sequence classification a
challenging task.
Sequence classification methods can be organized into three categories: (1) featurebased classification, which transforms a sequence into a feature vector and then applies
conventional classification methods; (2) sequence distance–based classification, where
the distance function that measures the similarity between sequences determines the

590

Chapter 13 Data Mining Trends and Research Frontiers

quality of the classification significantly; and (3) model-based classification such as using
hidden Markov model (HMM) or other statistical models to classify sequences.
For time-series or other numeric-valued data, the feature selection techniques for
symbolic sequences cannot be easily applied to time-series data without discretization.
However, discretization can cause information loss. A recently proposed time-series
shapelets method uses the time-series subsequences that can maximally represent a class
as the features. It achieves quality classification results.

Alignment of Biological Sequences
Biological sequences generally refer to sequences of nucleotides or amino acids. Biological sequence analysis compares, aligns, indexes, and analyzes biological sequences and
thus plays a crucial role in bioinformatics and modern biology.
Sequence alignment is based on the fact that all living organisms are related by evolution. This implies that the nucleotide (DNA, RNA) and protein sequences of species
that are closer to each other in evolution should exhibit more similarities. An alignment
is the process of lining up sequences to achieve a maximal identity level, which also
expresses the degree of similarity between sequences. Two sequences are homologous
if they share a common ancestor. The degree of similarity obtained by sequence alignment can be useful in determining the possibility of homology between two sequences.
Such an alignment also helps determine the relative positions of multiple species in an
evolution tree, which is called a phylogenetic tree.
The problem of alignment of biological sequences can be described as follows: Given
two or more input biological sequences, identify similar sequences with long conserved subsequences. If the number of sequences to be aligned is exactly two, the problem is known
as pairwise sequence alignment; otherwise, it is multiple sequence alignment. The
sequences to be compared and aligned can be either nucleotides (DNA/RNA) or amino
acids (proteins). For nucleotides, two symbols align if they are identical. However, for
amino acids, two symbols align if they are identical, or if one can be derived from the
other by substitutions that are likely to occur in nature. There are two kinds of alignments: local alignments and global alignments. The former means that only portions of
the sequences are aligned, whereas the latter requires alignment over the entire length of
the sequences.
For either nucleotides or amino acids, insertions, deletions, and substitutions occur
in nature with different probabilities. Substitution matrices are used to represent the
probabilities of substitutions of nucleotides or amino acids and probabilities of insertions and deletions. Usually, we use the gap character, −, to indicate positions where
it is preferable not to align two symbols. To evaluate the quality of alignments, a scoring mechanism is typically defined, which usually counts identical or similar symbols as
positive scores and gaps as negative ones. The algebraic sum of the scores is taken as the
alignment measure. The goal of alignment is to achieve the maximal score among all the
possible alignments. However, it is very expensive (more exactly, an NP-hard problem)
to find optimal alignment. Therefore, various heuristic methods have been developed to
find suboptimal alignments.

13.1 Mining Complex Data Types

591

The dynamic programming approach is commonly used for sequence alignments.
Among many available analysis packages, BLAST (Basic Local Alignment Search Tool)
is one of the most popular tools in biosequence analysis.

Hidden Markov Model for Biological Sequence Analysis
Given a biological sequence, biologists would like to analyze what that sequence represents. To represent the structure or statistical regularities of sequence classes, biologists
construct various probabilistic models such as Markov chains and hidden Markov
models. In both models, the probability of a state depends only on that of the previous
state; therefore, they are particularly useful for the analysis of biological sequence data.
The most common methods for constructing hidden Markov models are the forward
algorithm, the Viterbi algorithm, and the Baum-Welch algorithm. Given a sequence of
symbols, x, the forward algorithm finds the probability of obtaining x in the model; the
Viterbi algorithm finds the most probable path (corresponding to x) through the model,
whereas the Baum-Welch algorithm learns or adjusts the model parameters so as to best
explain a set of training sequences.

13.1.2 Mining Graphs and Networks
Graphs represents a more general class of structures than sets, sequences, lattices, and
trees. There is a broad range of graph applications on the Web and in social networks,
information networks, biological networks, bioinformatics, chemical informatics, computer vision, and multimedia and text retrieval. Hence, graph and network mining
have become increasingly important and heavily researched. We overview the following major themes: (1) graph pattern mining; (2) statistical modeling of networks;
(3) data cleaning, integration, and validation by network analysis; (4) clustering and
classification of graphs and homogeneous networks; (5) clustering, ranking, and classification of heterogeneous networks; (6) role discovery and link prediction in information
networks; (7) similarity search and OLAP in information networks; and (8) evolution
of information networks.

Graph Pattern Mining
Graph pattern mining is the mining of frequent subgraphs (also called (sub)graph patterns) in one or a set of graphs. Methods for mining graph patterns can be categorized
into Apriori-based and pattern growth–based approaches. Alternatively, we can mine
the set of closed graphs where a graph g is closed if there exists no proper supergraph
g 0 that carries the same support count as g. Moreover, there are many variant graph
patterns, including approximate frequent graphs, coherent graphs, and dense graphs.
User-specified constraints can be pushed deep into the graph pattern mining process to
improve mining efficiency.
Graph pattern mining has many interesting applications. For example, it can be
used to generate compact and effective graph index structures based on the concept of

592

Chapter 13 Data Mining Trends and Research Frontiers

frequent and discriminative graph patterns. Approximate structure similarity search can
be achieved by exploring graph index structures and multiple graph features. Moreover, classification of graphs can also be performed effectively using frequent and
discriminative subgraphs as features.

Statistical Modeling of Networks
A network consists of a set of nodes, each corresponding to an object associated with a set
of properties, and a set of edges (or links) connecting those nodes, representing relationships between objects. A network is homogeneous if all the nodes and links are of the
same type, such as a friend network, a coauthor network, or a web page network. A network is heterogeneous if the nodes and links are of different types, such as publication
networks (linking together authors, conferences, papers, and contents), and health-care
networks (linking together doctors, nurses, patients, diseases, and treatments).
Researchers have proposed multiple statistical models for modeling homogeneous
networks. The most well-known generative models are the random graph model (i.e.,
the Erdös-Rényi model), the Watts-Strogatz model, and the scale-free model. The scalefree model assumes that the network follows the power law distribution (also known
as the Pareto distribution or the heavy-tailed distribution). In most large-scale social
networks, a small-world phenomenon is observed, that is, the network can be characterized as having a high degree of local clustering for a small fraction of the nodes
(i.e., these nodes are interconnected with one another), while being no more than a few
degrees of separation from the remaining nodes.
Social networks exhibit certain evolutionary characteristics. They tend to follow the
densification power law, which states that networks become increasingly dense over
time. Shrinking diameter is another characteristic, where the effective diameter often
decreases as the network grows. Node out-degrees and in-degrees typically follow a heavytailed distribution.

Data Cleaning, Integration, and Validation
by Information Network Analysis
Real-world data are often incomplete, noisy, uncertain, and unreliable. Information
redundancy may exist among the multiple pieces of data that are interconnected in a
large network. Information redundancy can be explored in such networks to perform
quality data cleaning, data integration, information validation, and trustability analysis by network analysis. For example, we can distinguish authors who share the same
names by examining the networked connections with other heterogeneous objects such
as coauthors, publication venues, and terms. In addition, we can identify inaccurate
author information presented by booksellers by exploring a network built based on
author information provided by multiple booksellers.
Sophisticated information network analysis methods have been developed in this
direction, and in many cases, portions of the data serve as the “training set.” That
is, relatively clean and reliable data or a consensus of data from multiple information

13.1 Mining Complex Data Types

593

providers can be used to help consolidate the remaining, unreliable portions of the data.
This reduces the costly efforts of labeling the data by hand and of training on massive,
dynamic, real-world data sets.

Clustering and Classification of Graphs
and Homogeneous Networks
Large graphs and networks have cohesive structures, which are often hidden among
their massive, interconnected nodes and links. Cluster analysis methods have been developed on large networks to uncover network structures, discover hidden communities,
hubs, and outliers based on network topological structures and their associated properties. Various kinds of network clustering methods have been developed and can be
categorized as either partitioning, hierarchical, or density-based algorithms. Moreover,
given human-labeled training data, the discovery of network structures can be guided
by human-specified heuristic constraints. Supervised classification and semi-supervised
classification of networks are recent hot topics in the data mining research community.

Clustering, Ranking, and Classification
of Heterogeneous Networks
A heterogeneous network contains interconnected nodes and links of different types.
Such interconnected structures contain rich information, which can be used to mutually enhance nodes and links, and propagate knowledge from one type to another.
Clustering and ranking of such heterogeneous networks can be performed hand-inhand in the context that highly ranked nodes/links in a cluster may contribute more
than their lower-ranked counterparts in the evaluation of the cohesiveness of a cluster.
Clustering may help consolidate the high ranking of objects/links dedicated to the cluster. Such mutual enhancement of ranking and clustering prompted the development
of an algorithm called RankClus. Moreover, users may specify different ranking rules
or present labeled nodes/links for certain data types. Knowledge of one type can be
propagated to other types. Such propagation reaches the nodes/links of the same type
via heterogeneous-type connections. Algorithms have been developed for supervised
learning and semi-supervised learning in heterogeneous networks.

Role Discovery and Link Prediction
in Information Networks
There exist many hidden roles or relationships among different nodes/links in a heterogeneous network. Examples include advisor–advisee and leader–follower relationships
in a research publication network. To discover such hidden roles or relationships, experts
can specify constraints based on their background knowledge. Enforcing such constraints may help cross-checking and validation in large interconnected networks.
Information redundancy in a network can often be used to help weed out objects/links
that do not follow such constraints.

594

Chapter 13 Data Mining Trends and Research Frontiers

Similarly, link prediction can be performed based on the assessment of the ranking of the expected relationships among the candidate nodes/links. For example, we
may predict which papers an author may write, read, or cite, based on the author’s
recent publication history and the trend of research on similar topics. Such studies often
require analyzing the proximity of network nodes/links and the trends and connections
of their similar neighbors. Roughly speaking, people refer to link prediction as link
mining; however, link mining covers additional tasks including link-based object classification, object type prediction, link type prediction, link existence prediction, link cardinality
estimation, and object reconciliation (which predicts whether two objects are, in fact, the
same). It also includes group detection (which clusters objects), as well as subgraph identification (which finds characteristic subgraphs within networks) and metadata mining
(which uncovers schema-type information regarding unstructured data).

Similarity Search and OLAP in Information Networks
Similarity search is a primitive operation in database and web search engines. A heterogeneous information network consists of multityped, interconnected objects. Examples
include bibliographic networks and social media networks, where two objects are considered similar if they are linked in a similar way with multityped objects. In general,
object similarity within a network can be determined based on network structures
and object properties, and with similarity measures. Moreover, network clusters and
hierarchical network structures help organize objects in a network and identify subcommunities, as well as facilitate similarity search. Furthermore, similarity can be defined
differently per user. By considering different linkage paths, we can derive various
similarity semantics in a network, which is known as path-based similarity.
By organizing networks based on the notion of similarity and clusters, we can generate multiple hierarchies within a network. Online analytical processing (OLAP) can
then be performed. For example, we can drill down or dice information networks based
on different levels of abstraction and different angles of views. OLAP operations may
generate multiple, interrelated networks. The relationships among such networks may
disclose interesting hidden semantics.

Evolution of Social and Information Networks
Networks are dynamic and constantly evolving. Detecting evolving communities and
evolving regularities or anomalies in homogeneous or heterogeneous networks can help
people better understand the structural evolution of networks and predict trends and
irregularities in evolving networks. For homogeneous networks, the evolving communities discovered are subnetworks consisting of objects of the same type such as a set of
friends or coauthors. However, for heterogeneous networks, the communities discovered are subnetworks consisting of objects of different types, such as a connected set
of papers, authors, venues, and terms, from which we can also derive a set of evolving
objects for each type, like evolving authors and themes.

13.1 Mining Complex Data Types

595

13.1.3 Mining Other Kinds of Data
In addition to sequences and graphs, there are many other kinds of semi-structured
or unstructured data, such as spatiotemporal, multimedia, and hypertext data, which
have interesting applications. Such data carry various kinds of semantics, are either
stored in or dynamically streamed through a system, and call for specialized data mining
methodologies. Thus, mining multiple kinds of data, including spatial data, spatiotemporal data, cyber-physical system data, multimedia data, text data, web data, and data
streams, are increasingly important tasks in data mining. In this subsection, we overview
the methodologies for mining these kinds of data.

Mining Spatial Data
Spatial data mining discovers patterns and knowledge from spatial data. Spatial data,
in many cases, refer to geospace-related data stored in geospatial data repositories. The
data can be in “vector” or “raster” formats, or in the form of imagery and geo-referenced
multimedia. Recently, large geographic data warehouses have been constructed by integrating thematic and geographically referenced data from multiple sources. From these,
we can construct spatial data cubes that contain spatial dimensions and measures, and
support spatial OLAP for multidimensional spatial data analysis. Spatial data mining can
be performed on spatial data warehouses, spatial databases, and other geospatial data
repositories. Popular topics on geographic knowledge discovery and spatial data mining include mining spatial associations and co-location patterns, spatial clustering, spatial
classification, spatial modeling, and spatial trend and outlier analysis.

Mining Spatiotemporal Data and Moving Objects
Spatiotemporal data are data that relate to both space and time. Spatiotemporal data
mining refers to the process of discovering patterns and knowledge from spatiotemporal
data. Typical examples of spatiotemporal data mining include discovering the evolutionary history of cities and lands, uncovering weather patterns, predicting earthquakes and
hurricanes, and determining global warming trends. Spatiotemporal data mining has
become increasingly important and has far-reaching implications, given the popularity of mobile phones, GPS devices, Internet-based map services, weather services, and
digital Earth, as well as satellite, RFID, sensor, wireless, and video technologies.
Among many kinds of spatiotemporal data, moving-object data (i.e., data about moving objects) are especially important. For example, animal scientists attach telemetry
equipment on wildlife to analyze ecological behavior, mobility managers embed GPS
in cars to better monitor and guide vehicles, and meteorologists use weather satellites and radars to observe hurricanes. Massive-scale moving-object data are becoming
rich, complex, and ubiquitous. Examples of moving-object data mining include mining
movement patterns of multiple moving objects (i.e., the discovery of relationships among
multiple moving objects such as moving clusters, leaders and followers, merge, convoy,
swarm, and pincer, as well as other collective movement patterns). Other examples of

596

Chapter 13 Data Mining Trends and Research Frontiers

moving-object data mining include mining periodic patterns for one or a set of moving
objects, and mining trajectory patterns, clusters, models, and outliers.

Mining Cyber-Physical System Data
A cyber-physical system (CPS) typically consists of a large number of interacting
physical and information components. CPS systems may be interconnected so as to
form large heterogeneous cyber-physical networks. Examples of cyber-physical networks
include a patient care system that links a patient monitoring system with a network
of patient/medical information and an emergency handling system; a transportation
system that links a transportation monitoring network, consisting of many sensors and
video cameras, with a traffic information and control system; and a battlefield commander system that links a sensor/reconnaissance network with a battlefield information
analysis system. Clearly, cyber-physical systems and networks will be ubiquitous and
form a critical component of modern information infrastructure.
Data generated in cyber-physical systems are dynamic, volatile, noisy, inconsistent,
and interdependent, containing rich spatiotemporal information, and they are critically
important for real-time decision making. In comparison with typical spatiotemporal
data mining, mining cyber-physical data requires linking the current situation with
a large information base, performing real-time calculations, and returning prompt
responses. Research in the area includes rare-event detection and anomaly analysis in
cyber-physical data streams, reliability and trustworthiness in cyber-physical data analysis, effective spatiotemporal data analysis in cyber-physical networks, and the integration
of stream data mining with real-time automated control processes.

Mining Multimedia Data
Multimedia data mining is the discovery of interesting patterns from multimedia
databases that store and manage large collections of multimedia objects, including image
data, video data, audio data, as well as sequence data and hypertext data containing
text, text markups, and linkages. Multimedia data mining is an interdisciplinary field
that integrates image processing and understanding, computer vision, data mining, and
pattern recognition. Issues in multimedia data mining include content-based retrieval
and similarity search, and generalization and multidimensional analysis. Multimedia
data cubes contain additional dimensions and measures for multimedia information.
Other topics in multimedia mining include classification and prediction analysis, mining
associations, and video and audio data mining (Section 13.2.3).

Mining Text Data
Text mining is an interdisciplinary field that draws on information retrieval, data mining, machine learning, statistics, and computational linguistics. A substantial portion
of information is stored as text such as news articles, technical papers, books, digital
libraries, email messages, blogs, and web pages. Hence, research in text mining has been
very active. An important goal is to derive high-quality information from text. This is

13.1 Mining Complex Data Types

597

typically done through the discovery of patterns and trends by means such as statistical
pattern learning, topic modeling, and statistical language modeling. Text mining usually requires structuring the input text (e.g., parsing, along with the addition of some
derived linguistic features and the removal of others, and subsequent insertion into a
database). This is followed by deriving patterns within the structured data, and evaluation and interpretation of the output. “High quality” in text mining usually refers to a
combination of relevance, novelty, and interestingness.
Typical text mining tasks include text categorization, text clustering, concept/entity
extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity-relation modeling (i.e., learning relations between named entities).
Other examples include multilingual data mining, multidimensional text analysis, contextual text mining, and trust and evolution analysis in text data, as well as text mining
applications in security, biomedical literature analysis, online media analysis, and analytical customer relationship management. Various kinds of text mining and analysis
software and tools are available in academic institutions, open-source forums, and
industry. Text mining often also uses WordNet, Sematic Web, Wikipedia, and other
information sources to enhance the understanding and mining of text data.

Mining Web Data
The World Wide Web serves as a huge, widely distributed, global information center for
news, advertisements, consumer information, financial management, education, government, and e-commerce. It contains a rich and dynamic collection of information
about web page contents with hypertext structures and multimedia, hyperlink information, and access and usage information, providing fertile sources for data mining. Web
mining is the application of data mining techniques to discover patterns, structures, and
knowledge from the Web. According to analysis targets, web mining can be organized
into three main areas: web content mining, web structure mining, and web usage mining.
Web content mining analyzes web content such as text, multimedia data, and structured data (within web pages or linked across web pages). This is done to understand the
content of web pages, provide scalable and informative keyword-based page indexing,
entity/concept resolution, web page relevance and ranking, web page content summaries, and other valuable information related to web search and analysis. Web pages
can reside either on the surface web or on the deep Web. The surface web is that portion of the Web that is indexed by typical search engines. The deep Web (or hidden Web)
refers to web content that is not part of the surface web. Its contents are provided by
underlying database engines.
Web content mining has been studied extensively by researchers, search engines, and
other web service companies. Web content mining can build links across multiple web
pages for individuals; therefore, it has the potential to inappropriately disclose personal
information. Studies on privacy-preserving data mining address this concern through
the development of techniques to protect personal privacy on the Web.
Web structure mining is the process of using graph and network mining theory
and methods to analyze the nodes and connection structures on the Web. It extracts
patterns from hyperlinks, where a hyperlink is a structural component that connects a

598

Chapter 13 Data Mining Trends and Research Frontiers

web page to another location. It can also mine the document structure within a page
(e.g., analyze the treelike structure of page structures to describe HTML or XML tag
usage). Both kinds of web structure mining help us understand web contents and may
also help transform web contents into relatively structured data sets.
Web usage mining is the process of extracting useful information (e.g., user click
streams) from server logs. It finds patterns related to general or particular groups of
users; understands users’ search patterns, trends, and associations; and predicts what
users are looking for on the Internet. It helps improve search efficiency and effectiveness,
as well as promotes products or related information to different groups of users at the
right time. Web search companies routinely conduct web usage mining to improve their
quality of service.

Mining Data Streams
Stream data refer to data that flow into a system in vast volumes, change dynamically,
are possibly infinite, and contain multidimensional features. Such data cannot be stored
in traditional database systems. Moreover, most systems may only be able to read the
stream once in sequential order. This poses great challenges for the effective mining
of stream data. Substantial research has led to progress in the development of efficient methods for mining data streams, in the areas of mining frequent and sequential
patterns, multidimensional analysis (e.g., the construction of stream cubes), classification, clustering, outlier analysis, and the online detection of rare events in data streams.
The general philosophy is to develop single-scan or a-few-scan algorithms using limited
computing and storage capabilities.
This includes collecting information about stream data in sliding windows or tilted
time windows (where the most recent data are registered at the finest granularity and
the more distant data are registered at a coarser granularity), and exploring techniques
like microclustering, limited aggregation, and approximation. Many applications of
stream data mining can be explored—for example, real-time detection of anomalies in
computer network traffic, botnets, text streams, video streams, power-grid flows, web
searches, sensor networks, and cyber-physical systems.

13.2

Other Methodologies of Data Mining
Due to the broad scope of data mining and the large variety of data mining methodologies, not all methodologies of data mining can be thoroughly covered in this book.
In this section, we briefly discuss several interesting methodologies that were not fully
addressed in the previous chapters. These methodologies are listed in Figure 13.3.

13.2.1 Statistical Data Mining
The data mining techniques described in this book are primarily drawn from computer
science disciplines, including data mining, machine learning, data warehousing, and
algorithms. They are designed for the efficient handling of huge amounts of data that are

13.2 Other Methodologies of Data Mining

O
t
h
e
r
D
a
t
a
M
i
n
i
n
g
M
e
t
h
o
d
o
l
o
g
i
e
s

Statistical
Data Mining

Foundations
of Data Mining

Visual and Audio
Data Mining

599

Regression
Generalized linear models
Analysis of variance
Mixed-effect models
Factor analysis
Discriminant analysis
Survival analysis

Data reduction
Data compression
Probability and statistical theory
Microeconomic view
Pattern discovery and inductive database

Data visualization
Data mining result visualization
Data mining process visualization
Interactive visual data mining
Audio data mining

Figure 13.3 Other data mining methodologies.

typically multidimensional and possibly of various complex types. There are, however,
many well-established statistical techniques for data analysis, particularly for numeric
data. These techniques have been applied extensively to scientific data (e.g., data from
experiments in physics, engineering, manufacturing, psychology, and medicine), as well
as to data from economics and the social sciences. Some of these techniques, such as
principal components analysis (Chapter 3) and clustering (Chapters 10 and 11), have
already been addressed in this book. A thorough discussion of major statistical methods
for data analysis is beyond the scope of this book; however, several methods are mentioned here for the sake of completeness. Pointers to these techniques are provided in
the bibliographic notes (Section 13.8).
Regression: In general, these methods are used to predict the value of a response
(dependent) variable from one or more predictor (independent) variables, where the
variables are numeric. There are various forms of regression, such as linear, multiple, weighted, polynomial, nonparametric, and robust (robust methods are useful
when errors fail to satisfy normalcy conditions or when the data contain significant
outliers).
Generalized linear models: These models, and their generalization (generalized additive models), allow a categorical (nominal) response variable (or some transformation

600

Chapter 13 Data Mining Trends and Research Frontiers

of it) to be related to a set of predictor variables in a manner similar to the modeling of a numeric response variable using linear regression. Generalized linear models
include logistic regression and Poisson regression.
Analysis of variance: These techniques analyze experimental data for two or more
populations described by a numeric response variable and one or more categorical
variables (factors). In general, an ANOVA (single-factor analysis of variance) problem
involves a comparison of k population or treatment means to determine if at least two
of the means are different. More complex ANOVA problems also exist.
Mixed-effect models: These models are for analyzing grouped data—data that can
be classified according to one or more grouping variables. They typically describe
relationships between a response variable and some covariates in data grouped
according to one or more factors. Common areas of application include multilevel
data, repeated measures data, block designs, and longitudinal data.
Factor analysis: This method is used to determine which variables are combined to
generate a given factor. For example, for many psychiatric data, it is not possible to
measure a certain factor of interest directly (e.g., intelligence); however, it is often
possible to measure other quantities (e.g., student test scores) that reflect the factor
of interest. Here, none of the variables is designated as dependent.
Discriminant analysis: This technique is used to predict a categorical response variable. Unlike generalized linear models, it assumes that the independent variables
follow a multivariate normal distribution. The procedure attempts to determine
several discriminant functions (linear combinations of the independent variables)
that discriminate among the groups defined by the response variable. Discriminant
analysis is commonly used in social sciences.
Survival analysis: Several well-established statistical techniques exist for survival
analysis. These techniques originally were designed to predict the probability that
a patient undergoing a medical treatment would survive at least to time t. Methods
for survival analysis, however, are also commonly applied to manufacturing settings
to estimate the life span of industrial equipment. Popular methods include KaplanMeier estimates of survival, Cox proportional hazards regression models, and their
extensions.
Quality control: Various statistics can be used to prepare charts for quality control,
such as Shewhart charts and CUSUM charts (both of which display group summary statistics). These statistics include the mean, standard deviation, range, count,
moving average, moving standard deviation, and moving range.

13.2.2 Views on Data Mining Foundations
Research on the theoretical foundations of data mining has yet to mature. A solid and
systematic theoretical foundation is important because it can help provide a coherent

13.2 Other Methodologies of Data Mining

601

framework for the development, evaluation, and practice of data mining technology.
Several theories for the basis of data mining include the following:
Data reduction: In this theory, the basis of data mining is to reduce the data representation. Data reduction trades accuracy for speed in response to the need to
obtain quick approximate answers to queries on very large databases. Data reduction techniques include singular value decomposition (the driving element behind
principal components analysis), wavelets, regression, log-linear models, histograms,
clustering, sampling, and the construction of index trees.
Data compression: According to this theory, the basis of data mining is to compress
the given data by encoding in terms of bits, association rules, decision trees, clusters,
and so on. Encoding based on the minimum description length principle states that
the “best” theory to infer from a data set is the one that minimizes the length of the
theory and of the data when encoded, using the theory as a predictor for the data.
This encoding is typically in bits.
Probability and statistical theory: According to this theory, the basis of data mining is to discover joint probability distributions of random variables, for example,
Bayesian belief networks or hierarchical Bayesian models.
Microeconomic view: The microeconomic view considers data mining as the task
of finding patterns that are interesting only to the extent that they can be used in
the decision-making process of some enterprise (e.g., regarding marketing strategies
and production plans). This view is one of utility, in which patterns are considered
interesting if they can be acted on. Enterprises are regarded as facing optimization
problems, where the object is to maximize the utility or value of a decision. In this
theory, data mining becomes a nonlinear optimization problem.
Pattern discovery and inductive databases: In this theory, the basis of data mining
is to discover patterns occurring in the data such as associations, classification models, sequential patterns, and so on. Areas such as machine learning, neural network,
association mining, sequential pattern mining, clustering, and several other subfields
contribute to this theory. A knowledge base can be viewed as a database consisting
of data and patterns. A user interacts with the system by querying the data and the
theory (i.e., patterns) in the knowledge base. Here, the knowledge base is actually an
inductive database.
These theories are not mutually exclusive. For example, pattern discovery can also
be seen as a form of data reduction or data compression. Ideally, a theoretical framework should be able to model typical data mining tasks (e.g., association, classification,
and clustering), have a probabilistic nature, be able to handle different forms of data,
and consider the iterative and interactive essence of data mining. Further efforts are
required to establish a well-defined framework for data mining that satisfies these
requirements.

602

Chapter 13 Data Mining Trends and Research Frontiers

13.2.3 Visual and Audio Data Mining
Visual data mining discovers implicit and useful knowledge from large data sets using
data and/or knowledge visualization techniques. The human visual system is controlled
by the eyes and brain, the latter of which can be thought of as a powerful, highly parallel
processing and reasoning engine containing a large knowledge base. Visual data mining
essentially combines the power of these components, making it a highly attractive and
effective tool for the comprehension of data distributions, patterns, clusters, and outliers
in data.
Visual data mining can be viewed as an integration of two disciplines: data visualization and data mining. It is also closely related to computer graphics, multimedia systems,
human–computer interaction, pattern recognition, and high-performance computing.
In general, data visualization and data mining can be integrated in the following ways:
Data visualization: Data in a database or data warehouse can be viewed at different granularity or abstraction levels, or as different combinations of attributes or
dimensions. Data can be presented in various visual forms, such as boxplots, 3-D
cubes, data distribution charts, curves, surfaces, and link graphs, as shown in the
data visualization section of Chapter 2. Figures 13.4 and 13.5 from StatSoft show

Figure 13.4 Boxplots showing multiple variable combinations in StatSoft. Source: www.statsoft.com.

13.2 Other Methodologies of Data Mining

603

Figure 13.5 Multidimensional data distribution analysis in StatSoft. Source: www.statsoft.com.

data distributions in multidimensional space. Visual display can help give users a
clear impression and overview of the data characteristics in a large data set.
Data mining result visualization: Visualization of data mining results is the presentation of the results or knowledge obtained from data mining in visual forms. Such
forms may include scatter plots and boxplots (Chapter 2), as well as decision trees,
association rules, clusters, outliers, and generalized rules. For example, scatter plots
are shown in Figure 13.6 from SAS Enterprise Miner. Figure 13.7, from MineSet,
uses a plane associated with a set of pillars to describe a set of association rules mined
from a database. Figure 13.8, also from MineSet, presents a decision tree. Figure 13.9,
from IBM Intelligent Miner, presents a set of clusters and the properties associated
with them.
Data mining process visualization: This type of visualization presents the various
processes of data mining in visual forms so that users can see how the data are
extracted and from which database or data warehouse they are extracted, as well as
how the selected data are cleaned, integrated, preprocessed, and mined. Moreover, it
may also show which method is selected for data mining, where the results are stored,
and how they may be viewed. Figure 13.10 shows a visual presentation of data mining
processes by the Clementine data mining system.

604

Chapter 13 Data Mining Trends and Research Frontiers

Figure 13.6 Visualization of data mining results in SAS Enterprise Miner.

Interactive visual data mining: In (interactive) visual data mining, visualization
tools can be used in the data mining process to help users make smart data mining
decisions. For example, the data distribution in a set of attributes can be displayed
using colored sectors (where the whole space is represented by a circle). This display helps users determine which sector should first be selected for classification
and where a good split point for this sector may be. An example of this is shown in
Figure 13.11, which is the output of a perception-based classification (PBC) system
developed at the University of Munich.
Audio data mining uses audio signals to indicate the patterns of data or the features
of data mining results. Although visual data mining may disclose interesting patterns
using graphical displays, it requires users to concentrate on watching patterns and identifying interesting or novel features within them. This can sometimes be quite tiresome.
If patterns can be transformed into sound and music, then instead of watching pictures, we can listen to pitchs, rhythm, tune, and melody to identify anything interesting
or unusual. This may relieve some of the burden of visual concentration and be more

13.2 Other Methodologies of Data Mining

Figure 13.7 Visualization of association rules in MineSet.

Figure 13.8 Visualization of a decision tree in MineSet.

605

606

Chapter 13 Data Mining Trends and Research Frontiers

Figure 13.9 Visualization of cluster groupings in IBM Intelligent Miner.

Figure 13.10 Visualization of data mining processes by Clementine.

13.3 Data Mining Applications

607

Figure 13.11 Perception-based classification, an interactive visual mining approach.

relaxing than visual mining. Therefore, audio data mining is an interesting complement
to visual mining.

13.3

Data Mining Applications
In this book, we have studied principles and methods for mining relational data, data
warehouses, and complex data types. Because data mining is a relatively young discipline
with wide and diverse applications, there is still a nontrivial gap between general principles of data mining and application-specific, effective data mining tools. In this section,
we examine several application domains, as listed in Figure 13.12. We discuss how
customized data mining methods and tools should be developed for such applications.

13.3.1 Data Mining for Financial Data Analysis
Most banks and financial institutions offer a wide variety of banking, investment, and
credit services (the latter include business, mortgage, and automobile loans and credit
cards). Some also offer insurance and stock investment services.

608

Chapter 13 Data Mining Trends and Research Frontiers

Financial
Data Analysis

Retail and
Telecommunication
Industries

Science and
Engineering

Data Mining
Applications

Intrusion
Detection
and Prevention

Recommender
Systems

Figure 13.12 Common data mining application domains.

Financial data collected in the banking and financial industry are often relatively
complete, reliable, and of high quality, which facilitates systematic data analysis and data
mining. Here we present a few typical cases.
Design and construction of data warehouses for multidimensional data analysis
and data mining: Like many other applications, data warehouses need to be constructed for banking and financial data. Multidimensional data analysis methods
should be used to analyze the general properties of such data. For example, a company’s financial officer may want to view the debt and revenue changes by month,
region, and sector, and other factors, along with maximum, minimum, total, average, trend, deviation, and other statistical information. Data warehouses, data cubes
(including advanced data cube concepts such as multifeature, discovery-driven,
regression, and prediction data cubes), characterization and class comparisons, clustering, and outlier analysis will all play important roles in financial data analysis and
mining.
Loan payment prediction and customer credit policy analysis: Loan payment prediction and customer credit analysis are critical to the business of a bank. Many
factors can strongly or weakly influence loan payment performance and customer
credit rating. Data mining methods, such as attribute selection and attribute relevance ranking, may help identify important factors and eliminate irrelevant ones.
For example, factors related to the risk of loan payments include loan-to-value ratio,
term of the loan, debt ratio (total amount of monthly debt versus total monthly
income), payment-to-income ratio, customer income level, education level, residence region, and credit history. Analysis of the customer payment history may find
that, say, payment-to-income ratio is a dominant factor, while education level and
debt ratio are not. The bank may then decide to adjust its loan-granting policy so

13.3 Data Mining Applications

609

as to grant loans to those customers whose applications were previously denied but
whose profiles show relatively low risks according to the critical factor analysis.
Classification and clustering of customers for targeted marketing: Classification
and clustering methods can be used for customer group identification and targeted
marketing. For example, we can use classification to identify the most crucial factors
that may influence a customer’s decision regarding banking. Customers with similar
behaviors regarding loan payments may be identified by multidimensional clustering
techniques. These can help identify customer groups, associate a new customer with
an appropriate customer group, and facilitate targeted marketing.
Detection of money laundering and other financial crimes: To detect money laundering and other financial crimes, it is important to integrate information from
multiple, heterogeneous databases (e.g., bank transaction databases and federal or
state crime history databases), as long as they are potentially related to the study.
Multiple data analysis tools can then be used to detect unusual patterns, such as large
amounts of cash flow at certain periods, by certain groups of customers. Useful tools
include data visualization tools (to display transaction activities using graphs by time
and by groups of customers), linkage and information network analysis tools (to
identify links among different customers and activities), classification tools (to filter unrelated attributes and rank the highly related ones), clustering tools (to group
different cases), outlier analysis tools (to detect unusual amounts of fund transfers
or other activities), and sequential pattern analysis tools (to characterize unusual
access sequences). These tools may identify important relationships and patterns
of activities and help investigators focus on suspicious cases for further detailed
examination.

13.3.2 Data Mining for Retail and Telecommunication Industries
The retail industry is a well-fit application area for data mining, since it collects huge
amounts of data on sales, customer shopping history, goods transportation, consumption, and service. The quantity of data collected continues to expand rapidly, especially
due to the increasing availability, ease, and popularity of business conducted on the Web,
or e-commerce. Today, most major chain stores also have web sites where customers
can make purchases online. Some businesses, such as Amazon.com (www.amazon.com),
exist solely online, without any brick-and-mortar (i.e., physical) store locations. Retail
data provide a rich source for data mining.
Retail data mining can help identify customer buying behaviors, discover customer
shopping patterns and trends, improve the quality of customer service, achieve better
customer retention and satisfaction, enhance goods consumption ratios, design more
effective goods transportation and distribution policies, and reduce the cost of business.
A few examples of data mining in the retail industry are outlined as follows:
Design and construction of data warehouses: Because retail data cover a wide spectrum (including sales, customers, employees, goods transportation, consumption,

610

Chapter 13 Data Mining Trends and Research Frontiers

and services), there can be many ways to design a data warehouse for this industry.
The levels of detail to include can vary substantially. The outcome of preliminary
data mining exercises can be used to help guide the design and development of data
warehouse structures. This involves deciding which dimensions and levels to include
and what preprocessing to perform to facilitate effective data mining.
Multidimensional analysis of sales, customers, products, time, and region: The
retail industry requires timely information regarding customer needs, product sales,
trends, and fashions, as well as the quality, cost, profit, and service of commodities.
It is therefore important to provide powerful multidimensional analysis and visualization tools, including the construction of sophisticated data cubes according to the
needs of data analysis. The advanced data cube structures introduced in Chapter 5
are useful in retail data analysis because they facilitate analysis on multidimensional
aggregates with complex conditions.
Analysis of the effectiveness of sales campaigns: The retail industry conducts sales
campaigns using advertisements, coupons, and various kinds of discounts and
bonuses to promote products and attract customers. Careful analysis of the effectiveness of sales campaigns can help improve company profits. Multidimensional
analysis can be used for this purpose by comparing the amount of sales and the number of transactions containing the sales items during the sales period versus those
containing the same items before or after the sales campaign. Moreover, association
analysis may disclose which items are likely to be purchased together with the items
on sale, especially in comparison with the sales before or after the campaign.
Customer retention—analysis of customer loyalty: We can use customer loyalty
card information to register sequences of purchases of particular customers. Customer loyalty and purchase trends can be analyzed systematically. Goods purchased
at different periods by the same customers can be grouped into sequences. Sequential
pattern mining can then be used to investigate changes in customer consumption or
loyalty and suggest adjustments on the pricing and variety of goods to help retain
customers and attract new ones.
Product recommendation and cross-referencing of items: By mining associations
from sales records, we may discover that a customer who buys a digital camera is
likely to buy another set of items. Such information can be used to form product
recommendations. Collaborative recommender systems (Section 13.3.5) use data mining techniques to make personalized product recommendations during live customer
transactions, based on the opinions of other customers. Product recommendations
can also be advertised on sales receipts, in weekly flyers, or on the Web to help
improve customer service, aid customers in selecting items, and increase sales. Similarly, information, such as “hot items this week” or attractive deals, can be displayed
together with the associative information to promote sales.
Fraudulent analysis and the identification of unusual patterns: Fraudulent activity
costs the retail industry millions of dollars per year. It is important to (1) identify
potentially fraudulent users and their atypical usage patterns; (2) detect attempts
to gain fraudulent entry or unauthorized access to individual and organizational

13.3 Data Mining Applications

611

accounts; and (3) discover unusual patterns that may need special attention. Many of
these patterns can be discovered by multidimensional analysis, cluster analysis, and
outlier analysis.
As another industry that handles huge amounts of data, the telecommunication
industry has quickly evolved from offering local and long-distance telephone services
to providing many other comprehensive communication services. These include cellular phone, smart phone, Internet access, email, text messages, images, computer and web
data transmissions, and other data traffic. The integration of telecommunication, computer network, Internet, and numerous other means of communication and computing
has been under way, changing the face of telecommunications and computing. This has
created a great demand for data mining to help understand business dynamics, identify
telecommunication patterns, catch fraudulent activities, make better use of resources,
and improve service quality.
Data mining tasks in telecommunications share many similarities with those in
the retail industry. Common tasks include constructing large-scale data warehouses,
performing multidimensional visualization, OLAP, and in-depth analysis of trends,
customer patterns, and sequential patterns. Such tasks contribute to business improvements, cost reduction, customer retention, fraud analysis, and sharpening the edges
of competition. There are many data mining tasks for which customized data mining
tools for telecommunication have been flourishing and are expected to play increasingly
important roles in business.
Data mining has been popularly used in many other industries, such as insurance,
manufacturing, and health care, as well as for the analysis of governmental and institutional administration data. Although each industry has its own characteristic data
sets and application demands, they share many common principles and methodologies. Therefore, through effective mining in one industry, we may gain experience and
methodologies that can be transferred to other industrial applications.

13.3.3 Data Mining in Science and Engineering
In the past, many scientific data analysis tasks tended to handle relatively small and
homogeneous data sets. Such data were typically analyzed using a “formulate hypothesis,
build model, and evaluate results” paradigm. In these cases, statistical techniques were
typically employed for their analysis (see Section 13.2.1). Massive data collection and
storage technologies have recently changed the landscape of scientific data analysis. Today, scientific data can be amassed at much higher speeds and lower costs.
This has resulted in the accumulation of huge volumes of high-dimensional data,
stream data, and heterogenous data, containing rich spatial and temporal information. Consequently, scientific applications are shifting from the “hypothesize-and-test”
paradigm toward a “collect and store data, mine for new hypotheses, confirm with data or
experimentation” process. This shift brings about new challenges for data mining.
Vast amounts of data have been collected from scientific domains (including geosciences, astronomy, meteorology, geology, and biological sciences) using sophisticated

612

Chapter 13 Data Mining Trends and Research Frontiers

telescopes, multispectral high-resolution remote satellite sensors, global positioning systems, and new generations of biological data collection and analysis technologies. Large
data sets are also being generated due to fast numeric simulations in various fields such
as climate and ecosystem modeling, chemical engineering, fluid dynamics, and structural mechanics. Here we look at some of the challenges brought about by emerging
scientific applications of data mining.
Data warehouses and data preprocessing: Data preprocessing and data warehouses
are critical for information exchange and data mining. Creating a warehouse often
requires finding means for resolving inconsistent or incompatible data collected
in multiple environments and at different time periods. This requires reconciling semantics, referencing systems, geometry, measurements, accuracy, and precision. Methods are needed for integrating data from heterogeneous sources and for
identifying events.
For instance, consider climate and ecosystem data, which are spatial and temporal and require cross-referencing geospatial data. A major problem in analyzing such
data is that there are too many events in the spatial domain but too few in the temporal domain. For example, El Nino events occur only every four to seven years, and
previous data on them might not have been collected as systematically as they are
today. Methods are also needed for the efficient computation of sophisticated spatial
aggregates and the handling of spatial-related data streams.
Mining complex data types: Scientific data sets are heterogeneous in nature. They
typically involve semi-structured and unstructured data, such as multimedia data
and georeferenced stream data, as well as data with sophisticated, deeply hidden
semantics (e.g., genomic and proteomic data). Robust and dedicated analysis methods are needed for handling spatiotemporal data, biological data, related concept
hierarchies, and complex semantic relationships. For example, in bioinformatics,
a research problem is to identify regulatory influences on genes. Gene regulation
refers to how genes in a cell are switched on (or off) to determine the cell’s functions. Different biological processes involve different sets of genes acting together
in precisely regulated patterns. Thus, to understand a biological process we need to
identify the participating genes and their regulators. This requires the development
of sophisticated data mining methods to analyze large biological data sets for clues
about regulatory influences on specific genes, by finding DNA segments (“regulatory
sequences”) mediating such influence.
Graph-based and network-based mining: It is often difficult or impossible to
model several physical phenomena and processes due to limitations of existing
modeling approaches. Alternatively, labeled graphs and networks may be used to
capture many of the spatial, topological, geometric, biological, and other relational
characteristics present in scientific data sets. In graph or network modeling, each
object to be mined is represented by a vertex in a graph, and edges between vertices represent relationships between objects. For example, graphs can be used to
model chemical structures, biological pathways, and data generated by numeric

13.3 Data Mining Applications

613

simulations such as fluid-flow simulations. The success of graph or network modeling, however, depends on improvements in the scalability and efficiency of many
graph-based data mining tasks such as classification, frequent pattern mining, and
clustering.
Visualization tools and domain-specific knowledge: High-level graphical user
interfaces and visualization tools are required for scientific data mining systems.
These should be integrated with existing domain-specific data and information systems to guide researchers and general users in searching for patterns, interpreting
and visualizing discovered patterns, and using discovered knowledge in their decision
making.
Data mining in engineering shares many similarities with data mining in science.
Both practices often collect massive amounts of data, and require data preprocessing,
data warehousing, and scalable mining of complex types of data. Both typically use
visualization and make good use of graphs and networks. Moreover, many engineering processes need real-time responses, and so mining data streams in real time often
becomes a critical component.
Massive amounts of human communication data pour into our daily life. Such communication exists in many forms, including news, blogs, articles, web pages, online
discussions, product reviews, twitters, messages, advertisements, and communications,
both on the Web and in various kinds of social networks. Hence, data mining in social
science and social studies has become increasingly popular. Moreover, user or reader
feedback regarding products, speeches, and articles can be analyzed to deduce general
opinions and sentiments on the views of those in society. The analysis results can be
used to predict trends, improve work, and help in decision making.
Computer science generates unique kinds of data. For example, computer programs
can be long, and their execution often generates huge-size traces. Computer networks
can have complex structures and the network flows can be dynamic and massive. Sensor
networks may generate large amounts of data with varied reliability. Computer systems
and databases can suffer from various kinds of attacks, and their system/data accessing
may raise security and privacy concerns. These unique kinds of data provide fertile land
for data mining.
Data mining in computer science can be used to help monitor system status,
improve system performance, isolate software bugs, detect software plagiarism, analyze
computer system faults, uncover network intrusions, and recognize system malfunctions. Data mining for software and system engineering can operate on static or dynamic
(i.e., stream-based) data, depending on whether the system dumps traces beforehand for
postanalysis or if it must react in real time to handle online data.
Various methods have been developed in this domain, which integrate and extend
methods from machine learning, data mining, software/system engineering, pattern
recognition, and statistics. Data mining in computer science is an active and rich domain
for data miners because of its unique challenges. It requires the further development
of sophisticated, scalable, and real-time data mining and software/system engineering
methods.

614

Chapter 13 Data Mining Trends and Research Frontiers

13.3.4 Data Mining for Intrusion Detection and Prevention
The security of our computer systems and data is at continual risk. The extensive growth
of the Internet and the increasing availability of tools and tricks for intruding and
attacking networks have prompted intrusion detection and prevention to become a
critical component of networked systems. An intrusion can be defined as any set of
actions that threaten the integrity, confidentiality, or availability of a network resource
(e.g., user accounts, file systems, system kernels, and so on). Intrusion detection systems and intrusion prevention systems both monitor network traffic and/or system
executions for malicious activities. However, the former produces reports whereas the
latter is placed in-line and is able to actively prevent/block intrusions that are detected.
The main functions of an intrusion prevention system are to identify malicious activity, log information about said activity, attempt to block/stop activity, and report
activity.
The majority of intrusion detection and prevention systems use either signaturebased detection or anomaly-based detection.
Signature-based detection: This method of detection utilizes signatures, which
are attack patterns that are preconfigured and predetermined by domain experts.
A signature-based intrusion prevention system monitors the network traffic for
matches to these signatures. Once a match is found, the intrusion detection system will report the anomaly and an intrusion prevention system will take additional
appropriate actions. Note that since the systems are usually quite dynamic, the signatures need to be updated laboriously whenever new software versions arrive or
changes in network configuration or other situations occur. Another drawback is
that such a detection mechanism can only identify cases that match the signatures.
That is, it is unable to detect new or previously unknown intrusion tricks.
Anomaly-based detection: This method builds models of normal network behavior
(called profiles) that are then used to detect new patterns that significantly deviate
from the profiles. Such deviations may represent actual intrusions or simply be new
behaviors that need to be added to the profiles. The main advantage of anomaly
detection is that it may detect novel intrusions that have not yet been observed. Typically, a human analyst must sort through the deviations to ascertain which represent
real intrusions. A limiting factor of anomaly detection is the high percentage of false
positives. New patterns of intrusion can be added to the set of signatures to enhance
signature-based detection.
Data mining methods can help an intrusion detection and prevention system to
enhance its performance in various ways as follows.
New data mining algorithms for intrusion detection: Data mining algorithms can
be used for both signature-based and anomaly-based detection. In signature-based
detection, training data are labeled as either “normal” or “intrusion.” A classifier can then be derived to detect known intrusions. Research in this area has

13.3 Data Mining Applications

615

included the application of classification algorithms, association rule mining, and
cost-sensitive modeling. Anomaly-based detection builds models of normal behavior and automatically detects significant deviations from it. Methods include the
application of clustering, outlier analysis, and classification algorithms and statistical approaches. The techniques used must be efficient and scalable, and capable of
handling network data of high volume, dimensionality, and heterogeneity.
Association, correlation, and discriminative pattern analyses help select and build
discriminative classifiers: Association, correlation, and discriminative pattern mining can be applied to find relationships between system attributes describing the
network data. Such information can provide insight regarding the selection of useful
attributes for intrusion detection. New attributes derived from aggregated data may
also be helpful such as summary counts of traffic matching a particular pattern.
Analysis of stream data: Due to the transient and dynamic nature of intrusions
and malicious attacks, it is crucial to perform intrusion detection in the data stream
environment. Moreover, an event may be normal on its own, but considered malicious if viewed as part of a sequence of events. Thus, it is necessary to study what
sequences of events are frequently encountered together, find sequential patterns, and
identify outliers. Other data mining methods for finding evolving clusters and building dynamic classification models in data streams are also necessary for real-time
intrusion detection.
Distributed data mining: Intrusions can be launched from several different locations and targeted to many different destinations. Distributed data mining methods
may be used to analyze network data from several network locations to detect these
distributed attacks.
Visualization and querying tools: Visualization tools should be available for viewing any anomalous patterns detected. Such tools may include features for viewing
associations, discriminative patterns, clusters, and outliers. Intrusion detection systems should also have a graphical user interface that allows security analysts to pose
queries regarding the network data or intrusion detection results.
In summary, computer systems are at continual risk of breaks in security. Data mining
technology can be used to develop strong intrusion detection and prevention systems,
which may employ signature-based or anomaly-based detection.

13.3.5 Data Mining and Recommender Systems
Today’s consumers are faced with millions of goods and services when shopping online.
Recommender systems help consumers by making product recommendations that are
likely to be of interest to the user such as books, CDs, movies, restaurants, online
news articles, and other services. Recommender systems may use either a contentbased approach, a collaborative approach, or a hybrid approach that combines both
content-based and collaborative methods.

616

Chapter 13 Data Mining Trends and Research Frontiers

The content-based approach recommends items that are similar to items the
user preferred or queried in the past. It relies on product features and textual item
descriptions. The collaborative approach (or collaborative filtering approach) may
consider a user’s social environment. It recommends items based on the opinions of
other customers who have similar tastes or preferences as the user. Recommender systems use a broad range of techniques from information retrieval, statistics, machine
learning, and data mining to search for similarities among items and customer preferences. Consider Example 13.1.
Example 13.1 Scenarios of using a recommender system. Suppose that you visit the web site of an
online bookstore (e.g., Amazon) with the intention of purchasing a book that you have
been wanting to read. You type in the name of the book. This is not the first time you
have visited the web site. You have browsed through it before and even made purchases
from it last Christmas. The web store remembers your previous visits, having stored click
stream information and information regarding your past purchases. The system displays
the description and price of the book you have just specified. It compares your interests
with other customers having similar interests and recommends additional book titles,
saying “Customers who bought the book you have specified also bought these other titles as
well.” From surveying the list, you see another title that sparks your interest and decide
to purchase that one as well.
Now suppose you go to another online store with the intention of purchasing a digital
camera. The system suggests additional items to consider based on previously mined
sequential patterns, such as “Customers who buy this kind of digital camera are likely to
buy a particular brand of printer, memory card, or photo editing software within three
months.” You decide to buy just the camera, without any additional items. A week later,
you receive coupons from the store regarding the additional items.
An advantage of recommender systems is that they provide personalization for customers of e-commerce, promoting one-to-one marketing. Amazon, a pioneer in the use
of collaborative recommender systems, offers “a personalized store for every customer”
as part of their marketing strategy. Personalization can benefit both consumers and the
company involved. By having more accurate models of their customers, companies gain
a better understanding of customer needs. Serving these needs can result in greater success regarding cross-selling of related products, upselling, product affinities, one-to-one
promotions, larger baskets, and customer retention.
The recommendation problem considers a set, C, of users and a set, S, of items. Let u
be a utility function that measures the usefulness of an item, s, to a user, c. The utility is
commonly represented by a rating and is initially defined only for items previously rated
by users. For example, when joining a movie recommendation system, users are typically
asked to rate several movies. The space C × S of all possible users and items is huge. The
recommendation system should be able to extrapolate from known to unknown ratings
so as to predict item–user combinations. Items with the highest predicted rating/utility
for a user are recommended to that user.

13.3 Data Mining Applications

617

“How is the utility of an item estimated for a user?” In content-based methods, it
is estimated based on the utilities assigned by the same user to other items that are
similar. Many such systems focus on recommending items containing textual information, such as web sites, articles, and news messages. They look for commonalities
among items. For movies, they may look for similar genres, directors, or actors. For
articles, they may look for similar terms. Content-based methods are rooted in information theory. They make use of keywords (describing the items) and user profiles
that contain information about users’ tastes and needs. Such profiles may be obtained
explicitly (e.g., through questionnaires) or learned from users’ transactional behavior
over time.
A collaborative recommender system tries to predict the utility of items for a user,
u, based on items previously rated by other users who are similar to u. For example,
when recommending books, a collaborative recommender system tries to find other
users who have a history of agreeing with u (e.g., they tend to buy similar books, or give
similar ratings for books). Collaborative recommender systems can be either memory
(or heuristic) based or model based.
Memory-based methods essentially use heuristics to make rating predictions based
on the entire collection of items previously rated by users. That is, the unknown rating
of an item–user combination can be estimated as an aggregate of ratings of the most
similar users for the same item. Typically, a k-nearest-neighbor approach is used, that is,
we find the k other users (or neighbors) that are most similar to our target user, u. Various approaches can be used to compute the similarity between users. The most popular
approaches use either Pearson’s correlation coefficient (Section 3.3.2) or cosine similarity (Section 2.4.7). A weighted aggregate can be used, which adjusts for the fact that
different users may use the rating scale differently. Model-based collaborative recommender systems use a collection of ratings to learn a model, which is then used to make
rating predictions. For example, probabilistic models, clustering (which finds clusters
of like-minded customers), Bayesian networks, and other machine learning techniques
have been used.
Recommender systems face major challenges such as scalability and ensuring quality recommendations to the consumer. For example, regarding scalability, collaborative
recommender systems must be able to search through millions of potential neighbors
in real time. If the site is using browsing patterns as indications of product preference, it may have thousands of data points for some of its customers. Ensuring quality
recommendations is essential to gain consumers’ trust. If consumers follow a system
recommendation but then do not end up liking the product, they are less likely to use
the recommender system again.
As with classification systems, recommender systems can make two types of errors:
false negatives and false positives. Here, false negatives are products that the system
fails to recommend, although the consumer would like them. False positives are products that are recommended, but which the consumer does not like. False positives
are less desirable because they can annoy or anger consumers. Content-based recommender systems are limited by the features used to describe the items they recommend.

618

Chapter 13 Data Mining Trends and Research Frontiers

Another challenge for both content-based and collaborative recommender systems is
how to deal with new users for which a buying history is not yet available.
Hybrid approaches integrate both content-based and collaborative methods to
achieve further improved recommendations. The Netflix Prize was an open competition held by an online DVD-rental service, with a payout of $1,000,000 for the best
recommender algorithm to predict user ratings for films, based on previous ratings.
The competition and other studies have shown that the predictive accuracy of a recommender system can be substantially improved when blending multiple predictors,
especially by using an ensemble of many substantially different methods, rather than
refining a single technique.
Collaborative recommender systems are a form of intelligent query answering,
which consists of analyzing the intent of a query and providing generalized, neighborhood, or associated information relevant to the query. For example, rather than simply
returning the book description and price in response to a customer’s query, returning
additional information that is related to the query but that was not explicitly asked for
(e.g., book evaluation comments, recommendations of other books, or sales statistics)
provides an intelligent answer to the same query.

13.4

Data Mining and Society
For most of us, data mining is part of our daily lives, although we may often be unaware
of its presence. Section 13.4.1 looks at several examples of “ubiquitous and invisible”
data mining, affecting everyday things from the products stocked at our local supermarket, to the ads we see while surfing the Internet, to crime prevention. Data mining
can offer the individual many benefits by improving customer service and satisfaction
as well as lifestyle, in general. However, it also has serious implications regarding one’s
right to privacy and data security. These issues are the topic of Section 13.4.2.

13.4.1 Ubiquitous and Invisible Data Mining
Data mining is present in many aspects of our daily lives, whether we realize it or not.
It affects how we shop, work, and search for information, and can even influence our
leisure time, health, and well-being. In this section, we look at examples of such ubiquitous (or ever-present) data mining. Several of these examples also represent invisible
data mining, in which “smart” software, such as search engines, customer-adaptive web
services (e.g., using recommender algorithms), “intelligent” database systems, email
managers, ticket masters, and so on, incorporates data mining into its functional
components, often unbeknownst to the user.
From grocery stores that print personalized coupons on customer receipts to online
stores that recommend additional items based on customer interests, data mining has
innovatively influenced what we buy, the way we shop, and our experience while shopping. One example is Wal-Mart, which has hundreds of millions of customers visiting
its tens of thousands of stores every week. Wal-Mart allows suppliers to access data on

13.4 Data Mining and Society

619

their products and perform analyses using data mining software. This allows suppliers
to identify customer buying patterns at different stores, control inventory and product placement, and identify new merchandizing opportunities. All of these affect which
items (and how many) end up on the stores’ shelves—something to think about the next
time you wander through the aisles at Wal-Mart.
Data mining has shaped the online shopping experience. Many shoppers routinely
turn to online stores to purchase books, music, movies, and toys. Recommender systems, discussed in Section 13.3.5, offer personalized product recommendations based
on the opinions of other customers. Amazon.com was at the forefront of using such a
personalized, data mining–based approach as a marketing strategy. It has observed that
in traditional brick-and-mortar stores, the hardest part is getting the customer into the
store. Once the customer is there, he or she is likely to buy something, since the cost
of going to another store is high. Therefore, the marketing for brick-and-mortar stores
tends to emphasize drawing customers in, rather than the actual in-store customer experience. This is in contrast to online stores, where customers can “walk out” and enter
another online store with just a click of the mouse. Amazon.com capitalized on this difference, offering a “personalized store for every customer.” They use several data mining
techniques to identify customer’s likes and make reliable recommendations.
While we are on the topic of shopping, suppose you have been doing a lot of buying
with your credit cards. Nowadays, it is not unusual to receive a phone call from one’s
credit card company regarding suspicious or unusual patterns of spending. Credit card
companies use data mining to detect fraudulent usage, saving billions of dollars a year.
Many companies increasingly use data mining for customer relationship management (CRM), which helps provide more customized, personal service addressing
individual customer’s needs, in lieu of mass marketing. By studying browsing and purchasing patterns on web stores, companies can tailor advertisements and promotions
to customer profiles, so that customers are less likely to be annoyed with unwanted
mass mailings or junk mail. These actions can result in substantial cost savings for
companies. The customers further benefit in that they are more likely to be notified
of offers that are actually of interest, resulting in less waste of personal time and greater
satisfaction.
Data mining has greatly influenced the ways in which people use computers, search
for information, and work. Once you get on the Internet, for example, you decide to
check your email. Unbeknownst to you, several annoying emails have already been
deleted, thanks to a spam filter that uses classification algorithms to recognize spam.
After processing your email, you go to Google (www.google.com), which provides access
to information from billions of web pages indexed on its server. Google is one of the
most popular and widely used Internet search engines. Using Google to search for
information has become a way of life for many people.
Google is so popular that it has even become a new verb in the English language,
meaning “to search for (something) on the Internet using the Google search engine or,
by extension, any comprehensive search engine.”1 You decide to type in some keywords
1 http://open-dictionary.com.

620

Chapter 13 Data Mining Trends and Research Frontiers

for a topic of interest. Google returns a list of web sites on your topic, mined, indexed,
and organized by a set of data mining algorithms including PageRank. Moreover, if you
type “Boston New York,” Google will show you bus and train schedules from Boston to
New York; however, a minor change to “Boston Paris” will lead to flight schedules from
Boston to Paris. Such smart offerings of information or services are likely based on the
frequent patterns mined from the click streams of many previous queries.
While you are viewing the results of your Google query, various ads pop up relating
to your query. Google’s strategy of tailoring advertising to match the user’s interests is
one of the typical services being explored by every Internet search provider. This also
makes you happier, because you are less likely to be pestered with irrelevant ads.
Data mining is omnipresent, as can be seen from these daily-encountered examples.
We could go on and on with such scenarios. In many cases, data mining is invisible,
as users may be unaware that they are examining results returned by data mining or
that their clicks are actually fed as new data into some data mining functions. For data
mining to become further improved and accepted as a technology, continuing research
and development are needed in the many areas mentioned as challenges throughout this
book. These include efficiency and scalability, increased user interaction, incorporation
of background knowledge and visualization techniques, effective methods for finding
interesting patterns, improved handling of complex data types and stream data, realtime data mining, web mining, and so on. In addition, the integration of data mining
into existing business and scientific technologies, to provide domain-specific data mining tools, will further contribute to the advancement of the technology. The success of
data mining solutions tailored for e-commerce applications, as opposed to generic data
mining systems, is an example.

13.4.2 Privacy, Security, and Social Impacts of Data Mining
With more and more information accessible in electronic forms and available on the
Web, and with increasingly powerful data mining tools being developed and put into
use, there are increasing concerns that data mining may pose a threat to our privacy
and data security. However, it is important to note that many data mining applications
do not even touch personal data. Prominent examples include applications involving
natural resources, the prediction of floods and droughts, meteorology, astronomy, geography, geology, biology, and other scientific and engineering data. Furthermore, most
studies in data mining research focus on the development of scalable algorithms and do
not involve personal data.
The focus of data mining technology is on the discovery of general or statistically
significant patterns, not on specific information regarding individuals. In this sense,
we believe that the real privacy concerns are with unconstrained access to individual
records, especially access to privacy-sensitive information such as credit card transaction
records, health-care records, personal financial records, biological traits, criminal/justice
investigations, and ethnicity. For the data mining applications that do involve personal
data, in many cases, simple methods such as removing sensitive IDs from data may
protect the privacy of most individuals. Nevertheless, privacy concerns exist wherever

13.4 Data Mining and Society

621

personally identifiable information is collected and stored in digital form, and data
mining programs are able to access such data, even during data preparation.
Improper or nonexistent disclosure control can be the root cause of privacy issues.
To handle such concerns, numerous data security-enhancing techniques have been
developed. In addition, there has been a great deal of recent effort on developing privacypreserving data mining methods. In this section, we look at some of the advances in
protecting privacy and data security in data mining.
“What can we do to secure the privacy of individuals while collecting and mining data?”
Many data security–enhancing techniques have been developed to help protect data.
Databases can employ a multilevel security model to classify and restrict data according
to various security levels, with users permitted access to only their authorized level. It has
been shown, however, that users executing specific queries at their authorized security
level can still infer more sensitive information, and that a similar possibility can occur
through data mining. Encryption is another technique in which individual data items
may be encoded. This may involve blind signatures (which build on public key encryption), biometric encryption (e.g., where the image of a person’s iris or fingerprint is used
to encode his or her personal information), and anonymous databases (which permit the
consolidation of various databases but limit access to personal information only to those
who need to know; personal information is encrypted and stored at different locations).
Intrusion detection is another active area of research that helps protect the privacy of
personal data.
Privacy-preserving data mining is an area of data mining research in response
to privacy protection in data mining. It is also known as privacy-enhanced or privacysensitive data mining. It deals with obtaining valid data mining results without disclosing
the underlying sensitive data values. Most privacy-preserving data mining methods use
some form of transformation on the data to perform privacy preservation. Typically, such
methods reduce the granularity of representation to preserve privacy. For example, they
may generalize the data from individual customers to customer groups. This reduction
in granularity causes loss of information and possibly of the usefulness of the data
mining results. This is the natural trade-off between information loss and privacy.
Privacy-preserving data mining methods can be classified into the following categories.
Randomization methods: These methods add noise to the data to mask some
attribute values of records. The noise added should be sufficiently large so that
individual record values, especially sensitive ones, cannot be recovered. However,
it should be added skillfully so that the final results of data mining are basically
preserved. Techniques are designed to derive aggregate distributions from the perturbed data. Subsequently, data mining techniques can be developed to work with
these aggregate distributions.
The k-anonymity and l-diversity methods: Both of these methods alter individual
records so that they cannot be uniquely identified. In the k-anonymity method, the
granularity of data representation is reduced sufficiently so that any given record
maps onto at least k other records in the data. It uses techniques like generalization
and suppression. The k-anonymity method is weak in that, if there is a homogeneity

622

Chapter 13 Data Mining Trends and Research Frontiers

of sensitive values within a group, then those values may be inferred for the altered
records. The l-diversity model was designed to handle this weakness by enforcing
intragroup diversity of sensitive values to ensure anonymization. The goal is to make
it sufficiently difficult for adversaries to use combinations of record attributes to
exactly identify individual records.
Distributed privacy preservation: Large data sets could be partitioned and distributed either horizontally (i.e., the data sets are partitioned into different subsets
of records and distributed across multiple sites) or vertically (i.e., the data sets are
partitioned and distributed by their attributes), or even in a combination of both.
While the individual sites may not want to share their entire data sets, they may
consent to limited information sharing with the use of a variety of protocols. The
overall effect of such methods is to maintain privacy for each individual object, while
deriving aggregate results over all of the data.
Downgrading the effectiveness of data mining results: In many cases, even though
the data may not be available, the output of data mining (e.g, association rules and
classification models) may result in violations of privacy. The solution could be to
downgrade the effectiveness of data mining by either modifying data or mining results,
such as hiding some association rules or slightly distorting some classification models.
Recently, researchers proposed new ideas in privacy-preserving data mining such as
the notion of differential privacy. The general idea is that, for any two data sets that
are close to one another (i.e., that differ only on a tiny data set such as a single element), a given differentially private algorithm will behave approximately the same on
both data sets. This definition gives a strong guarantee that the presence or absence of a
tiny data set (e.g., representing an individual) will not affect the final output of the query
significantly. Based on this notion, a set of differential privacy-preserving data mining
algorithms have been developed. Research in this direction is ongoing. We expect more
powerful privacy-preserving data publishing and data mining algorithms in the near
future.
Like any other technology, data mining can be misused. However, we must not lose
sight of all the benefits that data mining research can bring, ranging from insights
gained from medical and scientific applications to increased customer satisfaction by
helping companies better suit their clients’ needs. We expect that computer scientists,
policy experts, and counterterrorism experts will continue to work with social scientists,
lawyers, companies, and consumers to take responsibility in building solutions to ensure
data privacy protection and security. In this way, we may continue to reap the benefits of
data mining in terms of time and money savings and the discovery of new knowledge.

13.5

Data Mining Trends
The diversity of data, data mining tasks, and data mining approaches poses many challenging research issues in data mining. The development of efficient and effective data

13.5 Data Mining Trends

623

mining methods, systems and services, and interactive and integrated data mining environments is a key area of study. The use of data mining techniques to solve large or
sophisticated application problems is an important task for data mining researchers
and data mining system and application developers. This section describes some of the
trends in data mining that reflect the pursuit of these challenges.
Application exploration: Early data mining applications put a lot of effort into helping businesses gain a competitive edge. The exploration of data mining for businesses
continues to expand as e-commerce and e-marketing have become mainstream in the
retail industry. Data mining is increasingly used for the exploration of applications
in other areas such as web and text analysis, financial analysis, industry, government, biomedicine, and science. Emerging application areas include data mining for
counterterrorism and mobile (wireless) data mining. Because generic data mining
systems may have limitations in dealing with application-specific problems, we may
see a trend toward the development of more application-specific data mining systems and tools, as well as invisible data mining functions embedded in various kinds
of services.
Scalable and interactive data mining methods: In contrast with traditional data
analysis methods, data mining must be able to handle huge amounts of data efficiently and, if possible, interactively. Because the amount of data being collected
continues to increase rapidly, scalable algorithms for individual and integrated
data mining functions become essential. One important direction toward improving the overall efficiency of the mining process while increasing user interaction is
constraint-based mining. This provides users with added control by allowing the
specification and use of constraints to guide data mining systems in their search for
interesting patterns and knowledge.
Integration of data mining with search engines, database systems, data warehouse
systems, and cloud computing systems: Search engines, database systems, data
warehouse systems, and cloud computing systems are mainstream information processing and computing systems. It is important to ensure that data mining serves
as an essential data analysis component that can be smoothly integrated into such
an information processing environment. A data mining subsystem/service should be
tightly coupled with such systems as a seamless, unified framework or as an invisible
function. This will ensure data availability, data mining portability, scalability, high
performance, and an integrated information processing environment for multidimensional data analysis and exploration.
Mining social and information networks: Mining social and information networks and link analysis are critical tasks because such networks are ubiquitous and
complex. The development of scalable and effective knowledge discovery methods and applications for large numbers of network data is essential, as outlined in
Section 13.1.2.
Mining spatiotemporal, moving-objects, and cyber-physical systems: Cyberphysical systems as well as spatiotemporal data are mounting rapidly due to the

624

Chapter 13 Data Mining Trends and Research Frontiers

popular use of cellular phones, GPS, sensors, and other wireless equipment. As
outlined in Section 13.1.3, there are many challenging research issues realizing
real-time and effective knowledge discovery with such data.
Mining multimedia, text, and web data: As outlined in Section 13.1.3, mining such
kinds of data is a recent focus in data mining research. Great progress has been made,
yet there are still many open issues to be solved.
Mining biological and biomedical data: The unique combination of complexity,
richness, size, and importance of biological and biomedical data warrants special attention in data mining. Mining DNA and protein sequences, mining highdimensional microarray data, and biological pathway and network analysis are just
a few topics in this field. Other areas of biological data mining research include
mining biomedical literature, link analysis across heterogeneous biological data, and
information integration of biological data by data mining.
Data mining with software engineering and system engineering: Software programs and large computer systems have become increasingly bulky in size
sophisticated in complexity, and tend to originate from the integration of multiple
components developed by different implementation teams. This trend has made it
an increasingly challenging task to ensure software robustness and reliability. The
analysis of the executions of a buggy software program is essentially a data mining
process—tracing the data generated during program executions may disclose important patterns and outliers that could lead to the eventual automated discovery of
software bugs. We expect that the further development of data mining methodologies for software/system debugging will enhance software robustness and bring new
vigor to software/system engineering.
Visual and audio data mining: Visual and audio data mining is an effective way
to integrate with humans’ visual and audio systems and discover knowledge from
huge amounts of data. A systematic development of such techniques will facilitate
the promotion of human participation for effective and efficient data analysis.
Distributed data mining and real-time data stream mining: Traditional data mining methods, designed to work at a centralized location, do not work well in
many of the distributed computing environments present today (e.g., the Internet, intranets, local area networks, high-speed wireless networks, sensor networks,
and cloud computing). Advances in distributed data mining methods are expected.
Moreover, many applications involving stream data (e.g., e-commerce, Web mining,
stock analysis, intrusion detection, mobile data mining, and data mining for counterterrorism) require dynamic data mining models to be built in real time. Additional
research is needed in this direction.
Privacy protection and information security in data mining: An abundance of
personal or confidential information available in electronic forms, coupled with
increasingly powerful data mining tools, poses a threat to data privacy and security.
Growing interest in data mining for counterterrorism also adds to the concern.

13.6 Summary

625

Further development of privacy-preserving data mining methods is foreseen. The
collaboration of technologists, social scientists, law experts, governments, and
companies is needed to produce a rigorous privacy and security protection mechanism for data publishing and data mining.
With confidence, we look forward to the next generation of data mining technology
and the further benefits that it will bring.

13.6

Summary
Mining complex data types poses challenging issues, for which there are many dedicated lines of research and development. This chapter presents a high-level overview
of mining complex data types, which includes mining sequence data such as time
series, symbolic sequences, and biological sequences; mining graphs and networks;
and mining other kinds of data, including spatiotemporal and cyber-physical system
data, multimedia, text and Web data, and data streams.
Several well-established statistical methods have been proposed for data analysis
such as regression, generalized linear models, analysis of variance, mixed-effect models, factor analysis, discriminant analysis, survival analysis, and quality control. Full
coverage of statistical data analysis methods is beyond the scope of this book. Interested readers are referred to the statistical literature cited in the bibliographic notes
(Section 13.8).
Researchers have been striving to build theoretical foundations for data mining. Several interesting proposals have appeared, based on data reduction, data compression,
probability and statistics theory, microeconomic theory, and pattern discovery–based
inductive databases.
Visual data mining integrates data mining and data visualization to discover implicit
and useful knowledge from large data sets. Visual data mining includes data visualization, data mining result visualization, data mining process visualization, and
interactive visual data mining. Audio data mining uses audio signals to indicate data
patterns or features of data mining results.
Many customized data mining tools have been developed for domain-specific
applications, including finance, the retail and telecommunication industries, science
and engineering, intrusion detection and prevention, and recommender systems.
Such application domain-based studies integrate domain-specific knowledge with
data analysis techniques and provide mission-specific data mining solutions.
Ubiquitous data mining is the constant presence of data mining in many aspects
of our daily lives. It can influence how we shop, work, search for information, and
use a computer, as well as our leisure time, health, and well-being. In invisible data
mining, “smart” software, such as search engines, customer-adaptive web services

626

Chapter 13 Data Mining Trends and Research Frontiers

(e.g., using recommender algorithms), email managers, and so on, incorporates data
mining into its functional components, often unbeknownst to the user.
A major social concern of data mining is the issue of privacy and data security.
Privacy-preserving data mining deals with obtaining valid data mining results without disclosing underlying sensitive values. Its goal is to ensure privacy protection and
security while preserving the overall quality of data mining results.
Data mining trends include further efforts toward the exploration of new application areas; improved scalable, interactive, and constraint-based mining methods; the
integration of data mining with web service, database, warehousing, and cloud computing systems; and mining social and information networks. Other trends include
the mining of spatiotemporal and cyber-physical system data, biological data, software/system engineering data, and multimedia and text data, in addition to web
mining, distributed and real-time data stream mining, visual and audio mining, and
privacy and security in data mining.

13.7

Exercises

13.1 Sequence data are ubiquitous and have diverse applications. This chapter presented a
general overview of sequential pattern mining, sequence classification, sequence similarity search, trend analysis, biological sequence alignment, and modeling. However,
we have not covered sequence clustering. Present an overview of methods for sequence
clustering.
13.2 This chapter presented an overview of sequence pattern mining and graph pattern
mining methods. Mining tree patterns and partial order patterns is also studied in
research. Summarize the methods for mining structured patterns, including sequences,
trees, graphs, and partial order relationships. Examine what kinds of structural pattern
mining have not been covered in research. Propose applications that can be created for
such new mining problems.
13.3 Many studies analyze homogeneous information networks (e.g., social networks consisting of friends linked with friends). However, many other applications involve heterogeneous information networks (i.e., networks linking multiple types of object such
as research papers, conference, authors, and topics). What are the major differences
between methodologies for mining heterogeneous information networks and methods
for their homogeneous counterparts?
13.4 Research and describe a data mining application that was not presented in this chapter.
Discuss how different forms of data mining can be used in the application.
13.5 Why is the establishment of theoretical foundations important for data mining? Name
and describe the main theoretical foundations that have been proposed for data mining. Comment on how they each satisfy (or fail to satisfy) the requirements of an ideal
theoretical framework for data mining.

13.7 Exercises

627

13.6 (Research project) Building a theory of data mining requires setting up a theoretical
framework so that the major data mining functions can be explained under this
framework. Take one theory as an example (e.g., data compression theory) and examine
how the major data mining functions fit into this framework. If some functions do not
fit well into the current theoretical framework, can you propose a way to extend the
framework to explain these functions?
13.7 There is a strong linkage between statistical data analysis and data mining. Some people
think of data mining as automated and scalable methods for statistical data analysis.
Do you agree or disagree with this perception? Present one statistical analysis method
that can be automated and/or scaled up nicely by integration with current data mining
methodology.
13.8 What are the differences between visual data mining and data visualization? Data visualization may suffer from the data abundance problem. For example, it is not easy to
visually discover interesting properties of network connections if a social network is
huge, with complex and dense connections. Propose a visualization method that may
help people see through the network topology to the interesting features of a social
network.
13.9 Propose a few implementation methods for audio data mining. Can we integrate audio
and visual data mining to bring fun and power to data mining? Is it possible to develop
some video data mining methods? State some scenarios and your solutions to make such
integrated audiovisual mining effective.
13.10 General-purpose computers and domain-independent relational database systems have
become a large market in the last several decades. However, many people feel that generic
data mining systems will not prevail in the data mining market. What do you think? For
data mining, should we focus our efforts on developing domain-independent data mining
tools or on developing domain-specific data mining solutions? Present your reasoning.
13.11 What is a recommender system? In what ways does it differ from a customer or productbased clustering system? How does it differ from a typical classification or predictive
modeling system? Outline one method of collaborative filtering. Discuss why it works
and what its limitations are in practice.
13.12 Suppose that your local bank has a data mining system. The bank has been studying
your debit card usage patterns. Noticing that you make many transactions at home
renovation stores, the bank decides to contact you, offering information regarding their
special loans for home improvements.
(a) Discuss how this may conflict with your right to privacy.
(b) Describe another situation in which you feel that data mining can infringe on your
privacy.
(c) Describe a privacy-preserving data mining method that may allow the bank to perform customer pattern analysis without infringing on its customers’ right to privacy.
(d) What are some examples where data mining could be used to help society? Can you
think of ways it could be used that may be detrimental to society?

628

Chapter 13 Data Mining Trends and Research Frontiers

13.13 What are the major challenges faced in bringing data mining research to market? Illustrate one data mining research issue that, in your view, may have a strong impact on the
market and on society. Discuss how to approach such a research issue.
13.14 Based on your view, what is the most challenging research problem in data mining? If
you were given a number of years and a good number of researchers and implementors,
what would your plan be to make good progress toward an effective solution to such a
problem?
13.15 Based on your experience and knowledge, suggest a new frontier in data mining that was
